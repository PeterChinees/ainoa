{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lZJpqv-38jmP7zCizHdy8mSu-IMlHlCZ",
      "authorship_tag": "ABX9TyOZQ3et2rqQGV2VkbUoQCBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/Attention_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaTkjkKu9TB",
        "colab_type": "code",
        "outputId": "72b82b64-d486-400a-9563-55024fabf9e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\r\u001b[K     |▌                               | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 28.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=4d287db5f62d2b47ffa391f928bf21bf5fc03d3e9f81c6cd606b2f3fd6dac7aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFhbGO8C4gJ",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xU5Pc6mvbY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import random\n",
        "import re\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from torchtext import data\n",
        "from torchtext.vocab import GloVe\n",
        "from transformers import BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzgKR2a8UeXi",
        "colab_type": "code",
        "outputId": "052ded61-5ef5-4a0d-bbaf-e5076fc57a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZYWV6HMDEm6",
        "colab_type": "text"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FczMGWp1vM_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    \"bert_tokenizer\": \"bert-base-uncased\",\n",
        "    \"bert_pretrained_model\": \"bert-base-uncased\",\n",
        "    \"distilbert_tokenizer\": \"distilbert-base-uncased\",\n",
        "    \"distilbert_pretrained_model\": \"distilbert-base-uncased\",\n",
        "    \"seed\": 1234,\n",
        "    \"bert_embedding_dim\": 768,\n",
        "    \"use_glove\": False,\n",
        "    \"glove_embedding_dim\": 300,\n",
        "    \"max_vocab_size\": 20000,\n",
        "    \"batch_size\": 10,\n",
        "    \"output_dim\": 11,\n",
        "    \"hidden_size\": 256,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.3,\n",
        "    \"fc_dropout\": 0.5,\n",
        "    \"embed_dropout\": 0.3,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"lr\": 0.001,\n",
        "    \"epochs\": 10\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLH3fXNGcq1L",
        "colab_type": "text"
      },
      "source": [
        "# Text pre-processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m57bXVOPcs0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessor(text):\n",
        "  text = re.sub('<[^>]*>', '', text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "          ' '.join(emoticons).replace('-', '')) \n",
        "  return text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s6Jmte7LiVS",
        "colab_type": "code",
        "outputId": "6707a43f-cc3c-4c5a-a768-9650d3819d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "og_text = '#Good music I love that #shit.'\n",
        "processed = preprocessor(og_text)\n",
        "processed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good', 'music', 'i', 'love', 'that', 'shit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QizRI-q1DPf1",
        "colab_type": "text"
      },
      "source": [
        "# Setup Bert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4nYNxu4vV9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(args['bert_tokenizer'])\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(args['distilbert_tokenizer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBv0eC2xvss-",
        "colab_type": "code",
        "outputId": "f079856b-b736-459f-ba19-ad382c4e825a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes[args['bert_tokenizer']]\n",
        "# max_input_length = tokenizer.max_model_input_sizes[args['distilbert_tokenizer']]\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMBjmcymvlY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(tweet):\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFFnMbqQDbJC",
        "colab_type": "text"
      },
      "source": [
        "# Load and Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYj3qsDpvmwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive'\n",
        "\n",
        "DATA_PATH = Path(file_path + '/datasets/SemEval')\n",
        "\n",
        "random.seed(args['seed'])\n",
        "np.random.seed(args['seed'])\n",
        "torch.manual_seed(args['seed'])\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "if args['use_glove']:\n",
        "  TEXT = data.Field(batch_first=True,\n",
        "                    tokenize=preprocessor,\n",
        "                    use_vocab=True,\n",
        "                    sequential=True)\n",
        "else:\n",
        "  TEXT = data.Field(batch_first = True,\n",
        "                use_vocab = False,\n",
        "                tokenize = tokenize,\n",
        "                preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                init_token = tokenizer.cls_token_id,\n",
        "                eos_token = tokenizer.sep_token_id,\n",
        "                pad_token = tokenizer.pad_token_id,\n",
        "                unk_token = tokenizer.unk_token_id)\n",
        "  \n",
        "LABEL = data.LabelField(sequential = False,\n",
        "                        use_vocab = False,\n",
        "                        pad_token= None,\n",
        "                        unk_token = None, \n",
        "                        dtype = torch.float)\n",
        "\n",
        "dataFields = {\"Tweet\": (\"Tweet\", TEXT),\n",
        "              'anger': (\"anger\", LABEL),\n",
        "              'anticipation': (\"anticipation\", LABEL),\n",
        "              'disgust': (\"disgust\", LABEL),\n",
        "              'fear': (\"fear\", LABEL),\n",
        "              'joy': (\"joy\", LABEL),\n",
        "              'love': (\"love\", LABEL),\n",
        "              'optimism': (\"optimism\", LABEL),\n",
        "              'pessimism': (\"pessimism\", LABEL),\n",
        "              'sadness': (\"sadness\", LABEL),\n",
        "              'surprise': (\"surprise\", LABEL),\n",
        "              'trust': (\"trust\", LABEL)}\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path = DATA_PATH,\n",
        "    train = 'train.csv',\n",
        "    validation = 'val.csv',\n",
        "    test = 'test.csv',\n",
        "    format = 'csv',\n",
        "    fields = dataFields\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYMezWxUvyxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = args['batch_size'],\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRmJtf4xPHxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if args['use_glove']:\n",
        "  TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300), \n",
        "                   max_size=args['max_vocab_size'])\n",
        "  print(f\"\\nUnique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dgYzwnpDhrO",
        "colab_type": "text"
      },
      "source": [
        "# Batch Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZCExSovoy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "              'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
        "\n",
        "iaux = 0\n",
        "\n",
        "for batch in valid_iterator:\n",
        "  iaux += 1\n",
        "  aux = batch\n",
        "  aux2 = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "  if aux == 20: break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyMA1ST4DlLN",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77GLQlKxDnAx",
        "colab_type": "text"
      },
      "source": [
        "Load the pretrained bert model from the HuggingFace transformers library.\n",
        "\n",
        "https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRiJ1llOvvxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained(args['bert_pretrained_model'])\n",
        "# bert = DistilBertModel.from_pretrained(args['distilbert_pretrained_model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toQo8u76tf3I",
        "colab_type": "text"
      },
      "source": [
        "Use model architecture proposed at: https://www.aclweb.org/anthology/P16-2034/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD1MiUiwv29n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, H):\n",
        "    M = torch.tanh(H)\n",
        "    M = self.attention(M).squeeze(2)\n",
        "    alpha = F.softmax(M, dim=1).unsqueeze(1)\n",
        "    return alpha\n",
        "\n",
        "class AttentionBiLSTM(nn.Module):\n",
        "  def __init__(self, hidden_size, num_layers, dropout, fc_dropout, \n",
        "               emb_layer_dropout, num_classes):\n",
        "    super(AttentionBiLSTM, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    if args['use_glove']:\n",
        "      embedding_dim = args['glove_embedding_dim']\n",
        "      self.embedding = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
        "    else:\n",
        "      self.bert = bert\n",
        "      embedding_dim = args['bert_embedding_dim']\n",
        "    \n",
        "    # embedding layer dropout\n",
        "    self.emb_layer_dropout = nn.Dropout(emb_layer_dropout)\n",
        "\n",
        "    # lstm layer\n",
        "    self.lstm = nn.LSTM(embedding_dim, \n",
        "                        hidden_size, \n",
        "                        num_layers, \n",
        "                        dropout=(0 if num_layers==1 else dropout),\n",
        "                        bidirectional=True,\n",
        "                        batch_first=True)\n",
        "    \n",
        "    # penultimate layer\n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    self.fc_dropout = nn.Dropout(fc_dropout)\n",
        "    \n",
        "    self.attention = Attention(hidden_size)\n",
        "  \n",
        "  def forward(self, text):\n",
        "    if args['use_glove']:\n",
        "      embedded = self.embedding(text)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        embedded = self.bert(text)[0]\n",
        "\n",
        "    embedded = self.emb_layer_dropout(embedded)\n",
        "    y, _ = self.lstm(embedded)\n",
        "    y = y[:,:,:self.hidden_size] + y[:,:,self.hidden_size:]\n",
        "    alpha = self.attention(y)\n",
        "    r = alpha.bmm(y).squeeze(1)\n",
        "    h = torch.tanh(r)\n",
        "    logits = self.fc(h)\n",
        "    logits = self.fc_dropout(logits)\n",
        "    return logits, alpha "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukvWtx9gxE1d",
        "colab_type": "code",
        "outputId": "25564087-ddf5-44a6-c198-111163751034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = AttentionBiLSTM(\n",
        "    hidden_size=args['hidden_size'],\n",
        "    num_layers=args['num_layers'],\n",
        "    dropout=args['dropout'],\n",
        "    fc_dropout=args['fc_dropout'],\n",
        "    emb_layer_dropout=args['embed_dropout'],\n",
        "    num_classes=args['output_dim'],\n",
        ")\n",
        "\n",
        "\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttentionBiLSTM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (emb_layer_dropout): Dropout(p=0.3, inplace=False)\n",
              "  (lstm): LSTM(768, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
              "  (fc_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (attention): Attention(\n",
              "    (attention): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HIm_dK8EbZ2",
        "colab_type": "text"
      },
      "source": [
        "Freeze the parameters which are a part of the Bert Transformers model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4i0FWYtxQ0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if args['use_glove'] is False:\n",
        "  for name, param in model.named_parameters():                \n",
        "      if name.startswith('bert'):\n",
        "          param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC9ZocNzEoDZ",
        "colab_type": "text"
      },
      "source": [
        "Show the trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1af7wW5xYk_",
        "colab_type": "code",
        "outputId": "ebe6d873-1329-4cce-d22b-f92d99cc7bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lstm.weight_ih_l0\n",
            "lstm.weight_hh_l0\n",
            "lstm.bias_ih_l0\n",
            "lstm.bias_hh_l0\n",
            "lstm.weight_ih_l0_reverse\n",
            "lstm.weight_hh_l0_reverse\n",
            "lstm.bias_ih_l0_reverse\n",
            "lstm.bias_hh_l0_reverse\n",
            "lstm.weight_ih_l1\n",
            "lstm.weight_hh_l1\n",
            "lstm.bias_ih_l1\n",
            "lstm.bias_hh_l1\n",
            "lstm.weight_ih_l1_reverse\n",
            "lstm.weight_hh_l1_reverse\n",
            "lstm.bias_ih_l1_reverse\n",
            "lstm.bias_hh_l1_reverse\n",
            "fc.weight\n",
            "fc.bias\n",
            "attention.attention.weight\n",
            "attention.attention.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLuzW7f4EyrX",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1CpxbIDxcb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwZIAAQmxfxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), weight_decay=args['weight_decay'])\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amV2jA0oE2Rx",
        "colab_type": "text"
      },
      "source": [
        "We evaluate using the Jaccard index and the macro and micro F1's as there are more suitable for multi-label text classification problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxF1sRahxmwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score, jaccard_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YydgwAqkxolz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metricize(preds, y):\n",
        "  f1_macro = f1_score(y, preds.round(), average='macro')\n",
        "  f1_micro = f1_score(y, preds.round(), average='micro')\n",
        "  #acc = roc_auc_score(y, preds)\n",
        "  jaccard = jaccard_score(y, preds.round(), average='samples')\n",
        "\n",
        "  return {\n",
        "      'f1_macro': f1_macro,\n",
        "      'f1_micro': f1_micro,\n",
        "      'jaccard': jaccard,\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLL3mSPZxpkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions, _ = model(batch.Tweet)\n",
        "\n",
        "    batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "    batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "    loss = criterion(predictions, batch_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "    labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLaQBVvxrla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions, _ = model(batch.Tweet)\n",
        "     \n",
        "      batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "      batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "      loss = criterion(predictions, batch_labels)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "      labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list)), preds_list, labels_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9vUx_1xtXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqaePMLEFP7e",
        "colab_type": "text"
      },
      "source": [
        "We train the model for 10 epochs and record the training and validation loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSBSFoBOxwJn",
        "colab_type": "code",
        "outputId": "0b481523-eecd-4d6d-936b-2a70de19c911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "train_acc = []\n",
        "valid_acc = []\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_metrics = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_metrics, _, _, = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        if args['use_glove']:\n",
        "          torch.save(model.state_dict(), 'glove-lstm-model.pt')\n",
        "        else:\n",
        "          torch.save(model.state_dict(), 'bert-lstm-model.pt')\n",
        "        \n",
        "    train_jaccard = train_metrics['jaccard']\n",
        "    train_micro = train_metrics['f1_micro']\n",
        "    train_macro = train_metrics['f1_macro']\n",
        "\n",
        "    valid_jaccard = valid_metrics['jaccard']\n",
        "    valid_micro = valid_metrics['f1_micro']\n",
        "    valid_macro = valid_metrics['f1_macro']\n",
        "\n",
        "    train_acc.append(train_jaccard)\n",
        "    valid_acc.append(valid_jaccard)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Jaccard: {train_jaccard*100:.2f}% | Train F1 Micro: {train_micro*100:.2f}% | Train F1 Macro: {train_macro*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Jaccard: {valid_jaccard*100:.2f}%  | Val. F1 Micro: {valid_micro*100:.2f}%  | Val. F1 Macro: {valid_macro*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.559 | Train Jaccard: 14.02% | Train F1 Micro: 23.92% | Train F1 Macro: 14.01%\n",
            "\t Val. Loss: 0.464 | Val. Jaccard: 36.10%  | Val. F1 Micro: 50.12%  | Val. F1 Macro: 31.18%\n",
            "Epoch: 02 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.533 | Train Jaccard: 21.41% | Train F1 Micro: 33.92% | Train F1 Macro: 22.13%\n",
            "\t Val. Loss: 0.417 | Val. Jaccard: 44.88%  | Val. F1 Micro: 57.24%  | Val. F1 Macro: 37.92%\n",
            "Epoch: 03 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.517 | Train Jaccard: 23.79% | Train F1 Micro: 36.87% | Train F1 Macro: 24.82%\n",
            "\t Val. Loss: 0.390 | Val. Jaccard: 47.94%  | Val. F1 Micro: 60.60%  | Val. F1 Macro: 41.16%\n",
            "Epoch: 04 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.507 | Train Jaccard: 25.55% | Train F1 Micro: 39.11% | Train F1 Macro: 27.98%\n",
            "\t Val. Loss: 0.384 | Val. Jaccard: 50.83%  | Val. F1 Micro: 64.21%  | Val. F1 Macro: 46.21%\n",
            "Epoch: 05 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.505 | Train Jaccard: 26.52% | Train F1 Micro: 40.46% | Train F1 Macro: 29.39%\n",
            "\t Val. Loss: 0.381 | Val. Jaccard: 50.39%  | Val. F1 Micro: 64.02%  | Val. F1 Macro: 47.77%\n",
            "Epoch: 06 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.501 | Train Jaccard: 26.92% | Train F1 Micro: 40.65% | Train F1 Macro: 30.27%\n",
            "\t Val. Loss: 0.371 | Val. Jaccard: 52.10%  | Val. F1 Micro: 65.22%  | Val. F1 Macro: 49.91%\n",
            "Epoch: 07 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.497 | Train Jaccard: 27.28% | Train F1 Micro: 41.46% | Train F1 Macro: 30.17%\n",
            "\t Val. Loss: 0.356 | Val. Jaccard: 55.16%  | Val. F1 Micro: 67.03%  | Val. F1 Macro: 51.62%\n",
            "Epoch: 08 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.494 | Train Jaccard: 28.55% | Train F1 Micro: 43.12% | Train F1 Macro: 33.07%\n",
            "\t Val. Loss: 0.365 | Val. Jaccard: 53.24%  | Val. F1 Micro: 65.78%  | Val. F1 Macro: 51.70%\n",
            "Epoch: 09 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.491 | Train Jaccard: 28.74% | Train F1 Micro: 43.15% | Train F1 Macro: 33.01%\n",
            "\t Val. Loss: 0.348 | Val. Jaccard: 55.57%  | Val. F1 Micro: 67.32%  | Val. F1 Macro: 49.58%\n",
            "Epoch: 10 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.488 | Train Jaccard: 29.33% | Train F1 Micro: 43.96% | Train F1 Macro: 34.42%\n",
            "\t Val. Loss: 0.358 | Val. Jaccard: 55.65%  | Val. F1 Micro: 67.04%  | Val. F1 Macro: 52.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqXexw71GcWj",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the training and validation loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CePfv7_0An7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgUKd4HR6Tg-",
        "colab_type": "code",
        "outputId": "860c21dd-9cc4-4223-d561-3daefa77234b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(train_losses)\n",
        "plt.plot(valid_losses)\n",
        "# plt.title('Attention LSTM (GloVe) Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training', 'Validation'])\n",
        "\n",
        "# plt.savefig('attn-lstm-glove', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f476ff8ac50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSlSdy6ALwj4",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the training and validation jaccard accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qDGgLbPL29w",
        "colab_type": "code",
        "outputId": "777bdf56-cc3b-4b35-af2a-a95d26524106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(train_acc)\n",
        "plt.plot(valid_acc)\n",
        "# plt.title('Attention LSTM (GloVe) Training & Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Jaccard')\n",
        "plt.legend(['Training', 'Validation'])\n",
        "# plt.savefig('attn-lstm-acc-glove', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f476f221550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtOPAqB1GkB5",
        "colab_type": "text"
      },
      "source": [
        "# Assess model performance on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TxQGreq6Vgm",
        "colab_type": "code",
        "outputId": "38508b4e-9ab8-41e5-8cf1-fa047bde1698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "if (args['use_glove']):\n",
        "  model.load_state_dict(torch.load('glove-lstm-model.pt'))\n",
        "else:\n",
        "  model.load_state_dict(torch.load('bert-lstm-model.pt'))\n",
        "\n",
        "\n",
        "test_loss, test_metrics, preds_list, labels_list = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "test_jaccard = test_metrics['jaccard']\n",
        "test_micro = test_metrics['f1_micro']\n",
        "test_macro = test_metrics['f1_macro']\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Jaccard: {test_jaccard*100:.2f}% | Test F1 Micro: {test_micro*100:.2f}% | Test F1 Macro: {test_macro*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.347 | Test Jaccard: 55.74% | Test F1 Micro: 67.50% | Test F1 Macro: 50.20%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJd_FzO4o_AK",
        "colab_type": "text"
      },
      "source": [
        "# Confusion matrix & Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXLsnO9-kFcf",
        "colab_type": "code",
        "outputId": "d8c33d3a-19e0-4bc5-c423-537f5d9043e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
        "\n",
        "conf_matrix = multilabel_confusion_matrix(np.vstack(labels_list), np.vstack(preds_list).round())\n",
        "print(conf_matrix)\n",
        "\n",
        "cm = classification_report(np.vstack(labels_list), np.vstack(preds_list).round())\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1961  197]\n",
            "  [ 327  774]]\n",
            "\n",
            " [[2797   37]\n",
            "  [ 385   40]]\n",
            "\n",
            " [[1903  257]\n",
            "  [ 323  776]]\n",
            "\n",
            " [[2710   64]\n",
            "  [ 219  266]]\n",
            "\n",
            " [[1585  232]\n",
            "  [ 252 1190]]\n",
            "\n",
            " [[2576  167]\n",
            "  [ 239  277]]\n",
            "\n",
            " [[1748  368]\n",
            "  [ 306  837]]\n",
            "\n",
            " [[2809   75]\n",
            "  [ 318   57]]\n",
            "\n",
            " [[2069  230]\n",
            "  [ 347  613]]\n",
            "\n",
            " [[3081    8]\n",
            "  [ 162    8]]\n",
            "\n",
            " [[3091   15]\n",
            "  [ 142   11]]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.70      0.75      1101\n",
            "           1       0.52      0.09      0.16       425\n",
            "           2       0.75      0.71      0.73      1099\n",
            "           3       0.81      0.55      0.65       485\n",
            "           4       0.84      0.83      0.83      1442\n",
            "           5       0.62      0.54      0.58       516\n",
            "           6       0.69      0.73      0.71      1143\n",
            "           7       0.43      0.15      0.22       375\n",
            "           8       0.73      0.64      0.68       960\n",
            "           9       0.50      0.05      0.09       170\n",
            "          10       0.42      0.07      0.12       153\n",
            "\n",
            "   micro avg       0.75      0.62      0.67      7869\n",
            "   macro avg       0.65      0.46      0.50      7869\n",
            "weighted avg       0.72      0.62      0.65      7869\n",
            " samples avg       0.73      0.64      0.65      7869\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaJzLdwb6exr",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ6W1Xpq6aO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emotion(model, tokenizer, tweet):\n",
        "  preds = []\n",
        "  model.eval()\n",
        "\n",
        "  if args['use_glove']:\n",
        "    tokenized = preprocessor(tweet)\n",
        "    indexed = [TEXT.vocab.stoi[token] for token in tokenized]\n",
        "  else:\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokens) + [tokenizer.sep_token_id]\n",
        "\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  predictions, attn_weights = model(tensor)\n",
        "  preds.append(torch.sigmoid(predictions).detach().cpu().numpy())\n",
        "\n",
        "  if args['use_glove']:\n",
        "    return preds, attn_weights, tokenized\n",
        "  else:\n",
        "    return preds, attn_weights, tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL9EtxTrG7My",
        "colab_type": "text"
      },
      "source": [
        "Lets test the model on our own input and save the attention weights and tokens for visualization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eecw3IVA6lVH",
        "colab_type": "code",
        "outputId": "59279427-abc5-4d97-f69e-46546981410a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "text = \"Good music I love that shit\"\n",
        "\n",
        "preds, attn_weights, tokens = predict_emotion(model, tokenizer, text)\n",
        "\n",
        "pred_values = []\n",
        "for p in preds[0]:\n",
        "  for val in p:\n",
        "    pred_values.append(val)\n",
        "\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "  print(f\"{label.upper()}: {pred_values[i]}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANGER: 0.16969524323940277\n",
            "ANTICIPATION: 0.3801114857196808\n",
            "DISGUST: 0.231175497174263\n",
            "FEAR: 0.45125430822372437\n",
            "JOY: 0.37219494581222534\n",
            "LOVE: 0.11220299452543259\n",
            "OPTIMISM: 0.526516854763031\n",
            "PESSIMISM: 0.38130509853363037\n",
            "SADNESS: 0.4075164198875427\n",
            "SURPRISE: 0.19744563102722168\n",
            "TRUST: 0.2266409695148468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jeDWh8KHbEG",
        "colab_type": "text"
      },
      "source": [
        "Here we format the attention weights and store the results in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q71LaaAn6oBA",
        "colab_type": "code",
        "outputId": "a92a585a-2355-40fd-8b3b-5a1c091bd930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "attention_weights = []\n",
        "for aw in attn_weights[0]:\n",
        "  for v in aw:\n",
        "    attention_weights.append(v.detach().cpu().numpy())\n",
        "\n",
        "if args['use_glove']:\n",
        "  attention_weights = np.array(attention_weights)\n",
        "else:\n",
        "  attention_weights = attention_weights[1:-1]\n",
        "  attention_weights = np.array(attention_weights)\n",
        "\n",
        "attention_weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00254789, 0.01076373, 0.01684024, 0.07311631, 0.03186547,\n",
              "       0.00803466, 0.00655064, 0.00805102, 0.006072  , 0.00671965,\n",
              "       0.00279574, 0.00230082, 0.00309695, 0.00623091, 0.01081251,\n",
              "       0.00539237, 0.00246369, 0.00121749, 0.00773399, 0.01135166,\n",
              "       0.02006537, 0.00754995, 0.00213846, 0.0062455 , 0.00142177,\n",
              "       0.00407601, 0.02648655, 0.01255506, 0.00345027, 0.00315737,\n",
              "       0.00614974, 0.00364961, 0.00330923, 0.00152066, 0.00148461,\n",
              "       0.00315663, 0.00177615, 0.00131094, 0.00086917, 0.00151383,\n",
              "       0.00099399, 0.00142413, 0.00168227, 0.0029159 , 0.00265497,\n",
              "       0.00214023, 0.00235863, 0.00307832, 0.00656858, 0.0074112 ,\n",
              "       0.00699761, 0.00281964, 0.00213332, 0.00229799, 0.00137846,\n",
              "       0.00245026, 0.00362945, 0.00184601, 0.00223766, 0.0024562 ,\n",
              "       0.00296904, 0.00319508, 0.0019913 , 0.0028417 , 0.00260649,\n",
              "       0.00242163, 0.0027601 , 0.00238864, 0.00289053, 0.00456507,\n",
              "       0.00244693, 0.00219304, 0.00211479, 0.00142508, 0.00162486,\n",
              "       0.00215474, 0.00235653, 0.00152567, 0.0067468 , 0.00145054,\n",
              "       0.01120737, 0.05180924, 0.02024387, 0.10658169, 0.00595807,\n",
              "       0.0128442 , 0.00117095, 0.00358747, 0.00183693, 0.00241286,\n",
              "       0.00135453, 0.00121846, 0.00631835, 0.00860316, 0.00259338,\n",
              "       0.00220318, 0.00097896, 0.00128085, 0.00103217, 0.00065362,\n",
              "       0.00108109, 0.0012942 , 0.00207685, 0.00139866, 0.00140768,\n",
              "       0.00326202, 0.00195968, 0.01917557, 0.09182516, 0.01838891,\n",
              "       0.01159031, 0.01423447, 0.03816426, 0.0040247 , 0.00515355,\n",
              "       0.00647479, 0.00571473, 0.01150566, 0.01963539, 0.01302929,\n",
              "       0.00771588, 0.00413715, 0.00312728, 0.00387638, 0.00424025,\n",
              "       0.00504196, 0.00293551], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnrDFOGy9DLx",
        "colab_type": "code",
        "outputId": "06d279e2-0667-491e-8771-af2fc61d5b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "attn_dict = {}\n",
        "for i in range(len(attention_weights)):\n",
        "  attn_dict[tokens[i]] = attention_weights[i]\n",
        "\n",
        "print(attn_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 0.005714731, \"'\": 0.0009789616, 'm': 0.01120737, 'only': 0.07311631, '24': 0.03186547, ',': 0.004024698, 'and': 0.005153555, 'have': 0.011505663, 'worked': 0.0060719987, 'part': 0.0020768465, 'time': 0.0013986574, 'jobs': 0.0013109408, 'since': 0.0030969537, 'graduated': 0.010812506, 'from': 0.0036496134, 'college': 0.0024636856, '.': 0.0019596778, 'with': 0.007733991, 'everything': 0.018388906, 'going': 0.020065365, 'on': 0.007549953, 've': 0.0040760064, 'been': 0.026486548, 'fur': 0.012555063, '##lou': 0.0034502705, '##gh': 0.0031573689, '##ed': 0.0061497386, 'both': 0.0033092287, 'of': 0.0015206585, 'my': 0.0010810941, 'well': 0.03816426, 'one': 0.0009939926, 'offered': 0.0014241328, 'me': 0.0016822703, 'full': 0.0036294525, 'was': 0.006568577, 'able': 0.0074112, 'to': 0.00065362296, 'negotiate': 0.0028196427, 'the': 0.00424025, 'salary': 0.0022979937, 'up': 0.001378462, 'a': 0.0016248557, 'dollar': 0.0018460079, 'per': 0.0022376643, 'hour': 0.0024562003, '!': 0.0029355057, 'it': 0.0024469285, 'has': 0.002841701, 'benefits': 0.002606489, 'know': 0.0045650653, 's': 0.0021147886, 'not': 0.0014250764, 'big': 0.002154737, 'deal': 0.0023565292, 'but': 0.0015256706, 'just': 0.019175567, 'so': 0.020243872, 'excited': 0.10658169, 'can': 0.012844201, 't': 0.0012808464, 'really': 0.0018369304, 'tell': 0.0024128626, 'anyone': 0.0013545282, 'especially': 0.006318355, 'as': 0.008603161, 'haven': 0.0022031826, 'spoken': 0.0010321654, 'other': 0.001294199, 'job': 0.0014076828, 'yet': 0.003262018, 'hoping': 0.091825165, 'works': 0.011590307, 'out': 0.014234467, 'that': 0.0064747864, 'money': 0.019635389, 'for': 0.013029293, 'gr': 0.007715879, '##ad': 0.0041371533, 'school': 0.003127283, 'in': 0.003876382, 'fall': 0.005041964}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomju0TvHp8z",
        "colab_type": "text"
      },
      "source": [
        "Lets return the top 3 words that the model focused on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skdcPb0g-nQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LqBqNKVCqan",
        "colab_type": "code",
        "outputId": "0e2ebcd1-9021-40bb-fa06-dcb7b4767e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "weight_counter = Counter(attn_dict)\n",
        "print(weight_counter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'excited': 0.10658169, 'hoping': 0.091825165, 'only': 0.07311631, 'well': 0.03816426, '24': 0.03186547, 'been': 0.026486548, 'so': 0.020243872, 'going': 0.020065365, 'money': 0.019635389, 'just': 0.019175567, 'everything': 0.018388906, 'out': 0.014234467, 'for': 0.013029293, 'can': 0.012844201, 'fur': 0.012555063, 'works': 0.011590307, 'have': 0.011505663, 'm': 0.01120737, 'graduated': 0.010812506, 'as': 0.008603161, 'with': 0.007733991, 'gr': 0.007715879, 'on': 0.007549953, 'able': 0.0074112, 'was': 0.006568577, 'that': 0.0064747864, 'especially': 0.006318355, '##ed': 0.0061497386, 'worked': 0.0060719987, 'i': 0.005714731, 'and': 0.005153555, 'fall': 0.005041964, 'know': 0.0045650653, 'the': 0.00424025, '##ad': 0.0041371533, 've': 0.0040760064, ',': 0.004024698, 'in': 0.003876382, 'from': 0.0036496134, 'full': 0.0036294525, '##lou': 0.0034502705, 'both': 0.0033092287, 'yet': 0.003262018, '##gh': 0.0031573689, 'school': 0.003127283, 'since': 0.0030969537, '!': 0.0029355057, 'has': 0.002841701, 'negotiate': 0.0028196427, 'benefits': 0.002606489, 'college': 0.0024636856, 'hour': 0.0024562003, 'it': 0.0024469285, 'tell': 0.0024128626, 'deal': 0.0023565292, 'salary': 0.0022979937, 'per': 0.0022376643, 'haven': 0.0022031826, 'big': 0.002154737, 's': 0.0021147886, 'part': 0.0020768465, '.': 0.0019596778, 'dollar': 0.0018460079, 'really': 0.0018369304, 'me': 0.0016822703, 'a': 0.0016248557, 'but': 0.0015256706, 'of': 0.0015206585, 'not': 0.0014250764, 'offered': 0.0014241328, 'job': 0.0014076828, 'time': 0.0013986574, 'up': 0.001378462, 'anyone': 0.0013545282, 'jobs': 0.0013109408, 'other': 0.001294199, 't': 0.0012808464, 'my': 0.0010810941, 'spoken': 0.0010321654, 'one': 0.0009939926, \"'\": 0.0009789616, 'to': 0.00065362296})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOynkS19gB9l",
        "colab_type": "text"
      },
      "source": [
        "# Model performance on longer text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaT0o3fUhjzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVRBt8P0H4Ur",
        "colab_type": "code",
        "outputId": "ac464ce5-82ea-42e0-e2db-3de8e1a9bfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "text_1 = (\n",
        "    \"I'm only 24, and have worked part time jobs since I graduated from college.\" \n",
        "    \" With everything going on, I've been furloughed from both of my part time jobs.\"\n",
        "    \" Well one offered me full time, and I was able to negotiate the salary up a full dollar per hour! And it has benefits and everything!\" \n",
        "    \" I know it's not a big deal but I'm just so excited and can't really tell anyone\"\n",
        "    \" especially as I haven't spoken to my other part time job yet. Just hoping everything works out well, and that I have money for grad school in the fall!\"\n",
        ")\n",
        "\n",
        "text_2 = (\n",
        "    \"I worked my ass off to graduate with my Bachelor's in 3 years.\"\n",
        "    \"My family hasn't said a word to me about it. No 'Congrats'! or 'I'm proud of you!'.\"\n",
        "    \"And it's not like they don't know. I live with them. I got my cap & gown in the mail last week and one my\"\n",
        "    \" professors snet me a graduation card. They were there for all of it. Still, not a single word about it.\"\n",
        "    \" I know this is a weird time, but I don't think that's an excuse to ignore your daughter's life achievements.\"\n",
        "    \" I feel so under-appreciated. I just want them to tell me they're proud. What kind of parent doesn't do that?\"\n",
        "    \" I'm 20 years old with a Bachelor of Science in Information Technology. I deserve a pat on the back.\"\n",
        "    \" This really sucks.\"\n",
        ")\n",
        "\n",
        "preds, attn_weights, tokens = predict_emotion(model, tokenizer, text_2)\n",
        "\n",
        "pred_values = []\n",
        "for p in preds[0]:\n",
        "  for val in p:\n",
        "    pred_values.append(val)\n",
        "\n",
        "# print(os.linesep.join([\"I'm only 24, and have worked part time jobs since I graduated from college.\", \n",
        "#                        \"With everything going on, I've been furloughed from both of my part time jobs.\",\n",
        "#                        \"Well one offered me full time, and I was able to negotiate the salary up a full dollar per hour!\", \n",
        "#                        \"And it has benefits and everything! I know it's not a big deal but I'm just so excited and\", \n",
        "#                        \"can't really tell anyone, especially as I haven't spoken to my other part time job yet.\", \n",
        "#                        \"Just hoping everything works out well, and that I have money for grad school in the fall!\"]))\n",
        "\n",
        "print(os.linesep.join([\"I worked my ass off to graduate with my Bachelor's in 3 years.\", \n",
        "                      \"My family hasn't said a word to me about it. No 'Congrats'! or 'I'm proud of you!'.\",\n",
        "                      \"And it's not like they don't know. I live with them. I got my cap & gown in the mail last week and one my\",\n",
        "                      \"professors snet me a graduation card. They were there for all of it. Still, not a single word about it.\",\n",
        "                      \"I know this is a weird time, but I don't think that's an excuse to ignore your daughter's life achievements.\",\n",
        "                      \"I feel so under-appreciated. I just want them to tell me they're proud. What kind of parent doesn't do that?\",\n",
        "                      \"I'm 20 years old with a Bachelor of Science in Information Technology. I deserve a pat on the back.\",\n",
        "                      \"This really sucks.\"]))\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Text length: 138 words\")\n",
        "\n",
        "print()\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "  print(f\"{label.upper()}: {pred_values[i]:.2f}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I worked my ass off to graduate with my Bachelor's in 3 years.\n",
            "My family hasn't said a word to me about it. No 'Congrats'! or 'I'm proud of you!'.\n",
            "And it's not like they don't know. I live with them. I got my cap & gown in the mail last week and one my\n",
            "professors snet me a graduation card. They were there for all of it. Still, not a single word about it.\n",
            "I know this is a weird time, but I don't think that's an excuse to ignore your daughter's life achievements.\n",
            "I feel so under-appreciated. I just want them to tell me they're proud. What kind of parent doesn't do that?\n",
            "I'm 20 years old with a Bachelor of Science in Information Technology. I deserve a pat on the back.\n",
            "This really sucks.\n",
            "\n",
            "Text length: 138 words\n",
            "\n",
            "ANGER: 0.28\n",
            "ANTICIPATION: 0.14\n",
            "DISGUST: 0.39\n",
            "FEAR: 0.22\n",
            "JOY: 0.33\n",
            "LOVE: 0.12\n",
            "OPTIMISM: 0.37\n",
            "PESSIMISM: 0.35\n",
            "SADNESS: 0.58\n",
            "SURPRISE: 0.13\n",
            "TRUST: 0.14\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}