{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDA0ADCq3RfDGkneZpKd+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/BERT_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owG-IxGmt6PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjzXVnDxtlRF",
        "colab_type": "code",
        "outputId": "fb4f7569-58cd-4a54-bf3d-9baead37145a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N2UGYjytpuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from torchtext import data\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR5RJneu1sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro_0OHbpu8G9",
        "colab_type": "code",
        "outputId": "ff7ec569-ed27-4cb7-d8e1-1bbc2600384a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeyvkR7XAqH4",
        "colab_type": "code",
        "outputId": "6c8337d3-0a20-41d8-95b3-bf12feb65a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEEo_OG7v4IN",
        "colab_type": "code",
        "outputId": "a11ec324-68cb-4834-e5b2-c95d1c9bcab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yUzVdafvcpC",
        "colab_type": "code",
        "outputId": "772e4492-ac78-4b5d-b426-72e4117121e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jx93Gzou92U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(tweet):\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwTeWoXTukDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive'\n",
        "\n",
        "DATA_PATH = Path(file_path + '/datasets/SemEval')\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential = False,\n",
        "                        use_vocab = False,\n",
        "                        pad_token= None,\n",
        "                        unk_token = None, \n",
        "                        dtype = torch.float)\n",
        "\n",
        "dataFields = {\"Tweet\": (\"Tweet\", TEXT),\n",
        "              'anger': (\"anger\", LABEL),\n",
        "              'anticipation': (\"anticipation\", LABEL),\n",
        "              'disgust': (\"disgust\", LABEL),\n",
        "              'fear': (\"fear\", LABEL),\n",
        "              'joy': (\"joy\", LABEL),\n",
        "              'love': (\"love\", LABEL),\n",
        "              'optimism': (\"optimism\", LABEL),\n",
        "              'pessimism': (\"pessimism\", LABEL),\n",
        "              'sadness': (\"sadness\", LABEL),\n",
        "              'surprise': (\"surprise\", LABEL),\n",
        "              'trust': (\"trust\", LABEL)}\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path = DATA_PATH,\n",
        "    train = 'train.csv',\n",
        "    validation = 'val.csv',\n",
        "    test = 'test.csv',\n",
        "    format = 'csv',\n",
        "    fields = dataFields\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0HXAS8EwK7z",
        "colab_type": "code",
        "outputId": "4bd7379c-7d05-44c4-8181-0b9ddff99d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 6838\n",
            "Number of validation examples: 886\n",
            "Number of testing examples: 3259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa2WXkBOwNN3",
        "colab_type": "code",
        "outputId": "f27333d3-9731-40b5-a79a-64c8bae630b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vars(train_data.examples[7]))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Tweet': [24246, 2015, 8239, 19662, 10875, 3085, 3475, 1005, 1056, 2002, 1029, 2042, 8239, 21794, 2153, 1010, 2139, 4502, 2100, 2038, 2246, 11519, 1006, 2003, 2232, 1007, 3892], 'anger': '1', 'anticipation': '0', 'disgust': '1', 'fear': '0', 'joy': '0', 'love': '0', 'optimism': '0', 'pessimism': '0', 'sadness': '0', 'surprise': '0', 'trust': '0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NXFqmEfwZjG",
        "colab_type": "code",
        "outputId": "21abd0cd-993b-4381-c485-71b6c00f3e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[7])['Tweet'])\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['rooney', '##s', 'fucking', 'unto', '##uch', '##able', 'isn', \"'\", 't', 'he', '?', 'been', 'fucking', 'dreadful', 'again', ',', 'de', '##pa', '##y', 'has', 'looked', 'decent', '(', 'is', '##h', ')', 'tonight']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz15EhbHwlTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf0eDnfJxEmF",
        "colab_type": "text"
      },
      "source": [
        "# Build Vocab for Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J749TkJ-Hth_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "              'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
        "\n",
        "iaux = 0\n",
        "\n",
        "for batch in valid_iterator:\n",
        "  iaux += 1\n",
        "  aux = batch\n",
        "  aux2 = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "  if aux == 20: break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBC9MytaIYfB",
        "colab_type": "code",
        "outputId": "aa940182-b4b7-4f99-d56c-00c6ddc4612f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "torch.transpose(torch.stack([getattr(aux, label) for label in LABEL_COLS]), 0, 1)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdVG6iO5xYSL",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Os2Jg6dxcHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe2-uC23xxhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertCNN(nn.Module):\n",
        "  def __init__(self, bert, n_filters, filter_sizes, output_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.bert = bert \n",
        "\n",
        "    embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "\n",
        "    self.convs = nn.ModuleList([\n",
        "                                nn.Conv2d(in_channels = 1,\n",
        "                                          out_channels = n_filters,\n",
        "                                          kernel_size = (fs, embedding_dim)) \n",
        "                                for fs in filter_sizes])\n",
        "    \n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, text):\n",
        "    with torch.no_grad():\n",
        "      embedded = self.bert(text)[0]\n",
        "    \n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    \n",
        "    conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    \n",
        "    cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    \n",
        "    return self.fc(cat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjxJgdC0umb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 11\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = BertCNN(bert, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKP5_3J30-qX",
        "colab_type": "code",
        "outputId": "86cb64be-5c6c-4228-bb67-837a3fee7db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertCNN(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(3, 768), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(4, 768), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(5, 768), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=300, out_features=11, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXDfpNUN0_Ns",
        "colab_type": "code",
        "outputId": "7de5d5dd-26ec-407a-e20d-a04665f33eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 110,407,451 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJmHEDeR1IUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMEjhKzy1MpT",
        "colab_type": "code",
        "outputId": "257458bb-9589-4d0d-9bd1-7c8ecbccd379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 925,211 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMffCB91RYi",
        "colab_type": "code",
        "outputId": "c64eb62f-d249-43a4-b770-1604a702f869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convs.0.weight\n",
            "convs.0.bias\n",
            "convs.1.weight\n",
            "convs.1.bias\n",
            "convs.2.weight\n",
            "convs.2.bias\n",
            "fc.weight\n",
            "fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TVLrvFb1Zch",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46iK1uys1Wmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQWnKk5m1eUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() \n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63IKcBC31rMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBCzL0Z-1vN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roc_auc(preds, y):\n",
        "  global var_y\n",
        "  global var_preds \n",
        "  var_y = y\n",
        "  var_preds = preds\n",
        "  acc = roc_auc_score(y, preds)\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vtKoYNR1xbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    predictions = model(batch.Tweet).squeeze(1)\n",
        "\n",
        "    batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "    batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "    loss = criterion(predictions, batch_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "    labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "    epoch_loss += loss.item() \n",
        "  \n",
        "  return epoch_loss / len(iterator), roc_auc(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nthY6Knv14uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "  epoch_acc = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions = model(batch.Tweet).squeeze(1)\n",
        "\n",
        "      batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "      batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "      loss = criterion(predictions, batch_labels)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "      labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "  return epoch_loss / len(iterator), roc_auc(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrqsUMQY17tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJRKTout1_BJ",
        "colab_type": "code",
        "outputId": "4e83506b-a0e8-4910-ae33-49950a348a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_history.append(train_loss)\n",
        "    valid_history.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'bert-cnn-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.427 | Train Acc: 67.58%\n",
            "\t Val. Loss: 0.354 |  Val. Acc: 81.59%\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.360 | Train Acc: 79.36%\n",
            "\t Val. Loss: 0.342 |  Val. Acc: 83.30%\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.337 | Train Acc: 82.41%\n",
            "\t Val. Loss: 0.333 |  Val. Acc: 83.79%\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.322 | Train Acc: 84.24%\n",
            "\t Val. Loss: 0.326 |  Val. Acc: 84.32%\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.312 | Train Acc: 85.15%\n",
            "\t Val. Loss: 0.320 |  Val. Acc: 84.46%\n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.301 | Train Acc: 86.67%\n",
            "\t Val. Loss: 0.326 |  Val. Acc: 84.68%\n",
            "Epoch: 07 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.294 | Train Acc: 87.38%\n",
            "\t Val. Loss: 0.322 |  Val. Acc: 84.58%\n",
            "Epoch: 08 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.286 | Train Acc: 88.16%\n",
            "\t Val. Loss: 0.328 |  Val. Acc: 84.65%\n",
            "Epoch: 09 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.276 | Train Acc: 89.12%\n",
            "\t Val. Loss: 0.325 |  Val. Acc: 84.54%\n",
            "Epoch: 10 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.270 | Train Acc: 89.74%\n",
            "\t Val. Loss: 0.325 |  Val. Acc: 84.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApekxv62E2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYvkc4ylPU2H",
        "colab_type": "code",
        "outputId": "d4407b32-b93e-4835-e4c9-ab4e1ab051fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(valid_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training', 'Validation'])"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f148d3a0630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4wxDbIEYYwxpkGWIIwxxjTo/wPenNtMLkEO7AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI8O3g5_PY1l",
        "colab_type": "code",
        "outputId": "ed70e39b-8a3b-4cf2-cb9e-512b8b78c146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('bert-cnn-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.316 | Test Acc: 83.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvYzUzo73i4",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldd6LfSYRJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emotion(model, tokenizer, tweet):\n",
        "  preds = []\n",
        "  model.eval()\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  predictions = model(tensor)\n",
        "  preds.append(torch.sigmoid(predictions).detach().cpu().numpy())\n",
        "  #preds += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0htQkUGZKmS",
        "colab_type": "code",
        "outputId": "af2276ef-068e-4282-cf19-b95c96c79114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "LABEL_COLS"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger',\n",
              " 'anticipation',\n",
              " 'disgust',\n",
              " 'fear',\n",
              " 'joy',\n",
              " 'love',\n",
              " 'optimism',\n",
              " 'pessimism',\n",
              " 'sadness',\n",
              " 'surprise',\n",
              " 'trust']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRlr-s2DY8Df",
        "colab_type": "code",
        "outputId": "7a447e28-eb6e-4e42-e716-d1a5c64efb3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "preds = predict_emotion(model, tokenizer, \"Do you think humans have the sense for recognizing impending doom?\")\n",
        "\n",
        "vals = []\n",
        "for p in preds[0]:\n",
        "  for val in p:\n",
        "    vals.append(val)\n",
        "\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "  print(f\"{label.upper()}: {vals[i]}\")"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANGER: 0.10812739282846451\n",
            "ANTICIPATION: 0.6953992247581482\n",
            "DISGUST: 0.28122904896736145\n",
            "FEAR: 0.3449912965297699\n",
            "JOY: 0.02500464953482151\n",
            "LOVE: 0.0022310586646199226\n",
            "OPTIMISM: 0.1092003658413887\n",
            "PESSIMISM: 0.2632802128791809\n",
            "SADNESS: 0.1401190161705017\n",
            "SURPRISE: 0.05643027648329735\n",
            "TRUST: 0.01979478821158409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ay0082n2i1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}