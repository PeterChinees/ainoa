{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8Pu6JtgSq+q/Adw20JRg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/BERT_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_svLHoq91gv6",
        "colab_type": "text"
      },
      "source": [
        "# Detecting Emotions from Tweets with BERT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjqbblbR2JTC",
        "colab_type": "text"
      },
      "source": [
        "First we need to install the transformers python package to get access to the pre-trained BERT models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BSjxdQA1su-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL3NJuV01YZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging \n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertTokenizer, BertPreTrainedModel, BertModel \n",
        "from transformers.optimization import AdamW \n",
        "from sklearn.metrics import f1_score \n",
        "from tqdm import tqdm_notebook as tqdm "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKcBftqH4ZLl",
        "colab_type": "text"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiOq1fU24Y2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiLabelBertClassifier(BertPreTrainedModel):\n",
        "  \"\"\"BERT model for multi-label classification\"\"\"\n",
        "\n",
        "  def __init__(self, config, num_labels=2):\n",
        "    super(MultiLabelBertClassifier, self).__init__(config)\n",
        "    self.num_labels = num_labels \n",
        "    self.bert = BertModel(config)\n",
        "    self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
        "    self.apply(self.init_bert_weights)\n",
        "\n",
        "  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "    _, pooled_output = self.bert(\n",
        "        input_ids, \n",
        "        token_type_ids, \n",
        "        attention_mask, \n",
        "        output_all_encoded_layers=False)\n",
        "    logits = self.classifier(pooled_output)\n",
        "\n",
        "    if labels is not None:\n",
        "      loss_func = nn.CrossEntropyLoss()\n",
        "      loss = loss_func(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
        "      return loss \n",
        "    else:\n",
        "      return logits \n",
        "  \n",
        "  def freeze_bert_encoder(self):\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "  \n",
        "  def unfreeze_bert_encoder(self):\n",
        "    for param in self.bert.parameters():\n",
        "      param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ueBFun2PSS",
        "colab_type": "text"
      },
      "source": [
        "# Data\n",
        "\n",
        "Lets import the Sem-Eval dataset and format it such that it can be fed into BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT1oNHtY1-LS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "bcc504e4-5ed2-4549-95c6-5d9a8b67ffeb"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4y-_Hhb2egi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(filename):\n",
        "  dataset = pd.read_csv(filename, sep='\\t')\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_5f8VzE2TfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7e09c4d1-592f-4161-ff61-77bd94009d06"
      },
      "source": [
        "file_path = '/content/drive/My Drive/datasets/'\n",
        "\n",
        "train_data = load_dataset(file_path + '2018-E-c-En-train.txt')\n",
        "validation_data = load_dataset(file_path + '2018-E-c-En-dev.txt')\n",
        "test_data = load_dataset(file_path + '2018-E-c-En-test-gold.txt')\n",
        "\n",
        "train_data.columns"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
              "       'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUT4X-Lg2pU-",
        "colab_type": "text"
      },
      "source": [
        "Our input data is the 'Tweet' column and label columns the emotion categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "786mrVgW2bUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'Tweet'\n",
        "LABEL_COLUMNS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "                 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jfnqHwN2s3u",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Here we transform the data into a format that BERT understands. This involves two steps. First, we modify the *InputExample* class to allow for multiple labels.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `labels` are the labels for our example, i.e. anger, disgust, fear, joy, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aen3I4aS2p7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    \"max_seq_length\": 128,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"warmup_steps\": 2000\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLbArWp8222g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "  def __init__(self, guid, text_a, text_b=None, labels=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            labels: (Optional) [string]. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.labels = labels \n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_ids = label_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJkJpQBU25ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, max_seq_length, \n",
        "                                 tokenizer, \n",
        "                                 labels_available=True):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            tokens = tokens[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        \n",
        "        if labels_available:\n",
        "            labels_ids = []\n",
        "            for label in example.labels:\n",
        "                labels_ids.append(label)    \n",
        "\n",
        "            features.append(\n",
        "                    InputFeatures(input_ids=input_ids,\n",
        "                                  input_mask=input_mask,\n",
        "                                  segment_ids=segment_ids,\n",
        "                                  label_ids=labels_ids))\n",
        "        else:\n",
        "            features.append(\n",
        "                    InputFeatures(input_ids=input_ids,\n",
        "                                  input_mask=input_mask,\n",
        "                                  segment_ids=segment_ids,))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btafc_8w38VF",
        "colab_type": "text"
      },
      "source": [
        "## Create a dataloader for training, testing and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VxZ3H-o3HEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataloader(data, batch_size, labels_available=True):\n",
        "  features = convert_examples_to_features(data, args['max_seq_length'], \n",
        "                                          tokenizer, labels_available)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}