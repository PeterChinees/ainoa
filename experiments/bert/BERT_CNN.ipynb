{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnVEx4oGSeFDkfju1l+LPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/BERT_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owG-IxGmt6PT",
        "colab_type": "code",
        "outputId": "9f081d24-2a8c-41ae-cbb2-124e7a9d3071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=5af2b898db8e9cece3e134c37971c77073ad282d28173b622cbafae8405d6389\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjzXVnDxtlRF",
        "colab_type": "code",
        "outputId": "0cea5786-9a40-4c87-940d-0422bcc53c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N2UGYjytpuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "3bbb47b8-823d-40b5-f220-dc27cfa930bb"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from torchtext import data\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR5RJneu1sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro_0OHbpu8G9",
        "colab_type": "code",
        "outputId": "d026c544-512b-40df-c718-368b443d3b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeyvkR7XAqH4",
        "colab_type": "code",
        "outputId": "c3ef20db-ffef-4d87-fb75-a9d1382fcfa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEEo_OG7v4IN",
        "colab_type": "code",
        "outputId": "b8a30ad8-6574-49bb-cd17-2b3fe0ee0710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yUzVdafvcpC",
        "colab_type": "code",
        "outputId": "712fa836-c81a-43a8-a12c-bbf87dddc35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jx93Gzou92U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(tweet):\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwTeWoXTukDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive'\n",
        "\n",
        "DATA_PATH = Path(file_path + '/datasets/SemEval')\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential = False,\n",
        "                        use_vocab = False,\n",
        "                        pad_token= None,\n",
        "                        unk_token = None, \n",
        "                        dtype = torch.float)\n",
        "\n",
        "dataFields = {\"Tweet\": (\"Tweet\", TEXT),\n",
        "              'anger': (\"anger\", LABEL),\n",
        "              'anticipation': (\"anticipation\", LABEL),\n",
        "              'disgust': (\"disgust\", LABEL),\n",
        "              'fear': (\"fear\", LABEL),\n",
        "              'joy': (\"joy\", LABEL),\n",
        "              'love': (\"love\", LABEL),\n",
        "              'optimism': (\"optimism\", LABEL),\n",
        "              'pessimism': (\"pessimism\", LABEL),\n",
        "              'sadness': (\"sadness\", LABEL),\n",
        "              'surprise': (\"surprise\", LABEL),\n",
        "              'trust': (\"trust\", LABEL)}\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path = DATA_PATH,\n",
        "    train = 'train.csv',\n",
        "    validation = 'val.csv',\n",
        "    test = 'test.csv',\n",
        "    format = 'csv',\n",
        "    fields = dataFields\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0HXAS8EwK7z",
        "colab_type": "code",
        "outputId": "3cd3ccda-04dc-4b50-869e-8ce84e4a2f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 6838\n",
            "Number of validation examples: 886\n",
            "Number of testing examples: 3259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa2WXkBOwNN3",
        "colab_type": "code",
        "outputId": "1ec00449-87db-48a3-f0bd-ebd9057ef635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(vars(train_data.examples[6]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Tweet': [2079, 2017, 2228, 4286, 2031, 1996, 3168, 2005, 14622, 17945, 12677, 1029], 'anger': '0', 'anticipation': '1', 'disgust': '0', 'fear': '0', 'joy': '0', 'love': '0', 'optimism': '0', 'pessimism': '1', 'sadness': '0', 'surprise': '0', 'trust': '0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NXFqmEfwZjG",
        "colab_type": "code",
        "outputId": "d4ba51e4-9a99-4e58-a0d7-8bf8104b20c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['Tweet'])\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['do', 'you', 'think', 'humans', 'have', 'the', 'sense', 'for', 'recognizing', 'impending', 'doom', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz15EhbHwlTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf0eDnfJxEmF",
        "colab_type": "text"
      },
      "source": [
        "# Build Vocab for Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J749TkJ-Hth_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "              'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
        "\n",
        "iaux = 0\n",
        "\n",
        "for batch in valid_iterator:\n",
        "  iaux += 1\n",
        "  aux = batch\n",
        "  aux2 = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "  if aux == 20: break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBC9MytaIYfB",
        "colab_type": "code",
        "outputId": "647c16de-7b0f-489e-e3b5-e9b702c79e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "torch.transpose(torch.stack([getattr(aux, label) for label in LABEL_COLS]), 0, 1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdVG6iO5xYSL",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Os2Jg6dxcHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe2-uC23xxhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertCNN(nn.Module):\n",
        "  def __init__(self, bert, n_filters, filter_sizes, output_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.bert = bert \n",
        "\n",
        "    embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "\n",
        "    self.convs = nn.ModuleList([\n",
        "                                nn.Conv2d(in_channels = 1,\n",
        "                                          out_channels = n_filters,\n",
        "                                          kernel_size = (fs, embedding_dim)) \n",
        "                                for fs in filter_sizes])\n",
        "    \n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, text):\n",
        "    with torch.no_grad():\n",
        "      embedded = self.bert(text)[0]\n",
        "    \n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    \n",
        "    conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    \n",
        "    cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    \n",
        "    return self.fc(cat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjxJgdC0umb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,3,1]\n",
        "OUTPUT_DIM = 11\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = BertCNN(bert, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKP5_3J30-qX",
        "colab_type": "code",
        "outputId": "a4402ade-cec5-4726-a4ba-ad79801ffe94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertCNN(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(3, 768), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(3, 768), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(1, 768), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=300, out_features=11, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXDfpNUN0_Ns",
        "colab_type": "code",
        "outputId": "07644c67-79a0-48d0-fcd3-780ab4d0e8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 110,023,451 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJmHEDeR1IUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMEjhKzy1MpT",
        "colab_type": "code",
        "outputId": "7691d5c6-657c-4047-dad7-1e9d86d34fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 541,211 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMffCB91RYi",
        "colab_type": "code",
        "outputId": "c7c97f52-e6b2-4c4b-8662-845d07bb1fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convs.0.weight\n",
            "convs.0.bias\n",
            "convs.1.weight\n",
            "convs.1.bias\n",
            "convs.2.weight\n",
            "convs.2.bias\n",
            "fc.weight\n",
            "fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TVLrvFb1Zch",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46iK1uys1Wmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQWnKk5m1eUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() \n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63IKcBC31rMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBCzL0Z-1vN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def roc_auc(preds, y):\n",
        "  global var_y\n",
        "  global var_preds \n",
        "  var_y = y\n",
        "  var_preds = preds\n",
        "  acc = roc_auc_score(y, preds)\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vtKoYNR1xbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    predictions = model(batch.Tweet).squeeze(1)\n",
        "\n",
        "    batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "    batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "    loss = criterion(predictions, batch_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "    labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "    epoch_loss += loss.item() \n",
        "  \n",
        "  return epoch_loss / len(iterator), roc_auc(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nthY6Knv14uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "  epoch_acc = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions = model(batch.Tweet).squeeze(1)\n",
        "\n",
        "      batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "      batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "      loss = criterion(predictions, batch_labels)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "      labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "  return epoch_loss / len(iterator), roc_auc(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrqsUMQY17tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJRKTout1_BJ",
        "colab_type": "code",
        "outputId": "94bd3718-a91f-4cf0-e2a8-cb32a31a8647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_history.append(train_acc)\n",
        "    valid_history.append(valid_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'bert-cnn-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.455 | Train Acc: 62.20%\n",
            "\t Val. Loss: 0.381 |  Val. Acc: 78.79%\n",
            "Epoch: 02 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.372 | Train Acc: 76.64%\n",
            "\t Val. Loss: 0.346 |  Val. Acc: 82.40%\n",
            "Epoch: 03 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.347 | Train Acc: 80.82%\n",
            "\t Val. Loss: 0.331 |  Val. Acc: 84.23%\n",
            "Epoch: 04 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.330 | Train Acc: 83.08%\n",
            "\t Val. Loss: 0.322 |  Val. Acc: 84.54%\n",
            "Epoch: 05 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.318 | Train Acc: 84.48%\n",
            "\t Val. Loss: 0.320 |  Val. Acc: 84.79%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApekxv62E2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYvkc4ylPU2H",
        "colab_type": "code",
        "outputId": "720677dd-ef15-4e4e-879d-ac11046e0ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(valid_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training', 'Validation'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f65baff19b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI8O3g5_PY1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5383ba68-24b9-4648-ef2a-fc4c64905be2"
      },
      "source": [
        "model.load_state_dict(torch.load('bert-cnn-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.313 | Test Acc: 84.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldd6LfSYRJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emotion(model, tokenizer, tweet):\n",
        "  preds = []\n",
        "  model.eval()\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  predictions = model(tensor)\n",
        "  preds += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0htQkUGZKmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "413d9f96-d911-47d1-e920-883411c8f2bc"
      },
      "source": [
        "LABEL_COLS"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger',\n",
              " 'anticipation',\n",
              " 'disgust',\n",
              " 'fear',\n",
              " 'joy',\n",
              " 'love',\n",
              " 'optimism',\n",
              " 'pessimism',\n",
              " 'sadness',\n",
              " 'surprise',\n",
              " 'trust']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRlr-s2DY8Df",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ef7f2222-3bcb-4f65-8c76-2110fa98659f"
      },
      "source": [
        "predict_emotion(model, tokenizer, \"Passed my exams, Whoop whoop!\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.01843822, 0.31698197, 0.02662484, 0.04498722, 0.9637813 ,\n",
              "         0.19794342, 0.66575414, 0.01522015, 0.03026438, 0.10788727,\n",
              "         0.07756213]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoH90p7wZAD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}