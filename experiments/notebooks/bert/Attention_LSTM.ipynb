{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lZJpqv-38jmP7zCizHdy8mSu-IMlHlCZ",
      "authorship_tag": "ABX9TyPOR4E2Pu8hmZA8147fqOWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/Attention_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaTkjkKu9TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFhbGO8C4gJ",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xU5Pc6mvbY0",
        "colab_type": "code",
        "outputId": "e6a9def0-c6d1-4bce-bc80-1c522b57ba59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from torchtext import data\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZYWV6HMDEm6",
        "colab_type": "text"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FczMGWp1vM_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    \"bert_tokenizer\": \"bert-base-uncased\",\n",
        "    \"bert_pretrained_model\": \"bert-base-uncased\",\n",
        "    \"seed\": 1234,\n",
        "    \"embedding_dim\": 768,\n",
        "    \"batch_size\": 64,\n",
        "    \"output_dim\": 11,\n",
        "    \"hidden_size\": 768,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.5,\n",
        "    \"fc_dropout\": 0.5,\n",
        "    \"embed_dropout\": 0.2,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"lr\": 0.001,\n",
        "    \"epochs\": 10\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QizRI-q1DPf1",
        "colab_type": "text"
      },
      "source": [
        "# Setup Bert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4nYNxu4vV9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(args['bert_tokenizer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBv0eC2xvss-",
        "colab_type": "code",
        "outputId": "96ff71e8-f6e2-4e4d-a869-12d0c8794db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes[args['bert_tokenizer']]\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMBjmcymvlY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(tweet):\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFFnMbqQDbJC",
        "colab_type": "text"
      },
      "source": [
        "# Load and Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYj3qsDpvmwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive'\n",
        "\n",
        "DATA_PATH = Path(file_path + '/datasets/SemEval')\n",
        "\n",
        "random.seed(args['seed'])\n",
        "np.random.seed(args['seed'])\n",
        "torch.manual_seed(args['seed'])\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = tokenizer.cls_token_id,\n",
        "                  eos_token = tokenizer.sep_token_id,\n",
        "                  pad_token = tokenizer.pad_token_id,\n",
        "                  unk_token = tokenizer.unk_token_id)\n",
        "\n",
        "LABEL = data.LabelField(sequential = False,\n",
        "                        use_vocab = False,\n",
        "                        pad_token= None,\n",
        "                        unk_token = None, \n",
        "                        dtype = torch.float)\n",
        "\n",
        "dataFields = {\"Tweet\": (\"Tweet\", TEXT),\n",
        "              'anger': (\"anger\", LABEL),\n",
        "              'anticipation': (\"anticipation\", LABEL),\n",
        "              'disgust': (\"disgust\", LABEL),\n",
        "              'fear': (\"fear\", LABEL),\n",
        "              'joy': (\"joy\", LABEL),\n",
        "              'love': (\"love\", LABEL),\n",
        "              'optimism': (\"optimism\", LABEL),\n",
        "              'pessimism': (\"pessimism\", LABEL),\n",
        "              'sadness': (\"sadness\", LABEL),\n",
        "              'surprise': (\"surprise\", LABEL),\n",
        "              'trust': (\"trust\", LABEL)}\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path = DATA_PATH,\n",
        "    train = 'train.csv',\n",
        "    validation = 'val.csv',\n",
        "    test = 'test.csv',\n",
        "    format = 'csv',\n",
        "    fields = dataFields\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYMezWxUvyxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = args['batch_size'],\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dgYzwnpDhrO",
        "colab_type": "text"
      },
      "source": [
        "# Batch Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZCExSovoy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "              'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
        "\n",
        "iaux = 0\n",
        "\n",
        "for batch in valid_iterator:\n",
        "  iaux += 1\n",
        "  aux = batch\n",
        "  aux2 = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "  if aux == 20: break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyMA1ST4DlLN",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77GLQlKxDnAx",
        "colab_type": "text"
      },
      "source": [
        "Load the pretrained bert model from the HuggingFace transformers library.\n",
        "\n",
        "https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRiJ1llOvvxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained(args['bert_pretrained_model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toQo8u76tf3I",
        "colab_type": "text"
      },
      "source": [
        "Setup the model architecture proposed at: https://www.aclweb.org/anthology/P16-2034/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD1MiUiwv29n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.tanh(x)\n",
        "    x = self.attention(x).squeeze(2)\n",
        "    alpha = F.softmax(x, dim=1).unsqueeze(1)\n",
        "    return alpha\n",
        "\n",
        "class AttentionBiLSTM(nn.Module):\n",
        "  def __init__(self, bert, hidden_size, num_layers, dropout, fc_dropout, \n",
        "               embed_dropout, num_classes):\n",
        "    super(AttentionBiLSTM, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size \n",
        "    self.bert = bert \n",
        "    embedding_dim = args['embedding_dim'] \n",
        "\n",
        "    self.embed_dropout = nn.Dropout(embed_dropout)\n",
        "\n",
        "    self.bilstm = nn.LSTM(embedding_dim, \n",
        "                          hidden_size, \n",
        "                          num_layers, \n",
        "                          dropout=(0 if num_layers==1 else dropout),\n",
        "                          bidirectional=True,\n",
        "                          batch_first=True)\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    self.fc_dropout = nn.Dropout(fc_dropout)\n",
        "    \n",
        "    self.attention = Attention(hidden_size)\n",
        "  \n",
        "  def forward(self, text):\n",
        "    with torch.no_grad():\n",
        "      x = self.bert(text)[0]\n",
        "    \n",
        "    x = self.embed_dropout(x)\n",
        "    y, _ = self.bilstm(x)\n",
        "    y = y[:,:,:self.hidden_size] + y[:,:,self.hidden_size:]\n",
        "    alpha = self.attention(y)\n",
        "    r = alpha.bmm(y).squeeze(1)\n",
        "    h = torch.tanh(r)\n",
        "    logits = self.fc(h)\n",
        "    logits = self.fc_dropout(logits)\n",
        "    return logits, alpha "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukvWtx9gxE1d",
        "colab_type": "code",
        "outputId": "24c771d9-d113-46dd-d8a1-d505f1dc55ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = AttentionBiLSTM(\n",
        "    bert=bert,\n",
        "    hidden_size=args['hidden_size'],\n",
        "    num_layers=args['num_layers'],\n",
        "    dropout=args['dropout'],\n",
        "    fc_dropout=args['fc_dropout'],\n",
        "    embed_dropout=args['embed_dropout'],\n",
        "    num_classes=args['output_dim'],\n",
        ")\n",
        "\n",
        "model"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttentionBiLSTM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (embed_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (bilstm): LSTM(768, 768, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=768, out_features=11, bias=True)\n",
              "  (fc_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (attention): Attention(\n",
              "    (attention): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HIm_dK8EbZ2",
        "colab_type": "text"
      },
      "source": [
        "Freeze the parameters which are a part of the Bert Transformers model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4i0FWYtxQ0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC9ZocNzEoDZ",
        "colab_type": "text"
      },
      "source": [
        "Show the trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1af7wW5xYk_",
        "colab_type": "code",
        "outputId": "73f7e7a6-d77b-4322-b740-490440dbc56b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bilstm.weight_ih_l0\n",
            "bilstm.weight_hh_l0\n",
            "bilstm.bias_ih_l0\n",
            "bilstm.bias_hh_l0\n",
            "bilstm.weight_ih_l0_reverse\n",
            "bilstm.weight_hh_l0_reverse\n",
            "bilstm.bias_ih_l0_reverse\n",
            "bilstm.bias_hh_l0_reverse\n",
            "bilstm.weight_ih_l1\n",
            "bilstm.weight_hh_l1\n",
            "bilstm.bias_ih_l1\n",
            "bilstm.bias_hh_l1\n",
            "bilstm.weight_ih_l1_reverse\n",
            "bilstm.weight_hh_l1_reverse\n",
            "bilstm.bias_ih_l1_reverse\n",
            "bilstm.bias_hh_l1_reverse\n",
            "fc.weight\n",
            "fc.bias\n",
            "attention.attention.weight\n",
            "attention.attention.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLuzW7f4EyrX",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1CpxbIDxcb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwZIAAQmxfxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), weight_decay=args['weight_decay'])\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amV2jA0oE2Rx",
        "colab_type": "text"
      },
      "source": [
        "We evaluate using the Jaccard index and the macro and micro F1's as there are more suitable for multi-label text classification problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxF1sRahxmwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score, jaccard_similarity_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YydgwAqkxolz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metricize(preds, y):\n",
        "  f1_macro = f1_score(y, preds.round(), average='macro')\n",
        "  f1_micro = f1_score(y, preds.round(), average='micro')\n",
        "  #acc = roc_auc_score(y, preds)\n",
        "  acc = jaccard_similarity_score(y, preds.round())\n",
        "\n",
        "  return {\n",
        "      'f1_macro': f1_macro,\n",
        "      'f1_micro': f1_micro,\n",
        "      'acc': acc\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLL3mSPZxpkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions, _ = model(batch.Tweet)\n",
        "\n",
        "    batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "    batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "    loss = criterion(predictions, batch_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "    labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLaQBVvxrla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions, _ = model(batch.Tweet)\n",
        "\n",
        "      batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "      batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "      loss = criterion(predictions, batch_labels)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "      labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9vUx_1xtXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqaePMLEFP7e",
        "colab_type": "text"
      },
      "source": [
        "We train the model for 10 epochs and record the training and validation loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSBSFoBOxwJn",
        "colab_type": "code",
        "outputId": "f89d9d59-c5d4-4501-cc13-58a510f40c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_metrics = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_metrics = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_history.append(train_loss)\n",
        "    valid_history.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'bert-lstm-model.pt')\n",
        "\n",
        "    train_acc = train_metrics['acc']\n",
        "    train_micro = train_metrics['f1_micro']\n",
        "    train_macro = train_metrics['f1_macro']\n",
        "\n",
        "    valid_acc = valid_metrics['acc']\n",
        "    valid_micro = valid_metrics['f1_micro']\n",
        "    valid_macro = valid_metrics['f1_macro']\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train F1 Micro: {train_micro*100:.2f}% | Train F1 Macro: {train_macro*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%  | Val. F1 Micro: {valid_micro*100:.2f}%  | Val. F1 Macro: {valid_macro*100:.2f}%')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:664: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 24s\n",
            "\tTrain Loss: 0.553 | Train Acc: 19.18% | Train F1 Micro: 28.78% | Train F1 Macro: 17.95%\n",
            "\t Val. Loss: 0.430 | Val. Acc: 45.44%  | Val. F1 Micro: 58.06%  | Val. F1 Macro: 35.76%\n",
            "Epoch: 02 | Epoch Time: 0m 24s\n",
            "\tTrain Loss: 0.525 | Train Acc: 24.55% | Train F1 Micro: 35.06% | Train F1 Macro: 23.23%\n",
            "\t Val. Loss: 0.392 | Val. Acc: 49.32%  | Val. F1 Micro: 61.92%  | Val. F1 Macro: 40.49%\n",
            "Epoch: 03 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.515 | Train Acc: 26.95% | Train F1 Micro: 38.24% | Train F1 Macro: 26.87%\n",
            "\t Val. Loss: 0.378 | Val. Acc: 49.28%  | Val. F1 Micro: 62.17%  | Val. F1 Macro: 40.42%\n",
            "Epoch: 04 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.506 | Train Acc: 27.71% | Train F1 Micro: 39.58% | Train F1 Macro: 28.10%\n",
            "\t Val. Loss: 0.383 | Val. Acc: 51.08%  | Val. F1 Micro: 64.49%  | Val. F1 Macro: 46.08%\n",
            "Epoch: 05 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.505 | Train Acc: 27.93% | Train F1 Micro: 39.84% | Train F1 Macro: 28.80%\n",
            "\t Val. Loss: 0.382 | Val. Acc: 53.34%  | Val. F1 Micro: 66.19%  | Val. F1 Macro: 52.43%\n",
            "Epoch: 06 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.500 | Train Acc: 28.90% | Train F1 Micro: 40.98% | Train F1 Macro: 29.89%\n",
            "\t Val. Loss: 0.383 | Val. Acc: 51.59%  | Val. F1 Micro: 64.74%  | Val. F1 Macro: 49.41%\n",
            "Epoch: 07 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.496 | Train Acc: 29.08% | Train F1 Micro: 41.46% | Train F1 Macro: 31.24%\n",
            "\t Val. Loss: 0.349 | Val. Acc: 55.12%  | Val. F1 Micro: 66.63%  | Val. F1 Macro: 47.49%\n",
            "Epoch: 08 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.494 | Train Acc: 29.75% | Train F1 Micro: 42.33% | Train F1 Macro: 31.99%\n",
            "\t Val. Loss: 0.369 | Val. Acc: 52.16%  | Val. F1 Micro: 64.86%  | Val. F1 Macro: 47.33%\n",
            "Epoch: 09 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.495 | Train Acc: 30.03% | Train F1 Micro: 42.36% | Train F1 Macro: 31.46%\n",
            "\t Val. Loss: 0.358 | Val. Acc: 54.59%  | Val. F1 Micro: 66.73%  | Val. F1 Macro: 48.94%\n",
            "Epoch: 10 | Epoch Time: 0m 25s\n",
            "\tTrain Loss: 0.490 | Train Acc: 30.76% | Train F1 Micro: 43.62% | Train F1 Macro: 32.65%\n",
            "\t Val. Loss: 0.352 | Val. Acc: 54.32%  | Val. F1 Micro: 66.68%  | Val. F1 Macro: 51.22%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqXexw71GcWj",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the training and validation loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CePfv7_0An7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgUKd4HR6Tg-",
        "colab_type": "code",
        "outputId": "51becad2-c8b1-4b8f-9ba5-a9ab257a8c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(valid_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training', 'Validation'])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2417f9e320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtOPAqB1GkB5",
        "colab_type": "text"
      },
      "source": [
        "# Assess model performance on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TxQGreq6Vgm",
        "colab_type": "code",
        "outputId": "5880f70b-1dbe-46c0-fc6a-178f0c54eea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "model.load_state_dict(torch.load('bert-lstm-model.pt'))\n",
        "\n",
        "test_loss, test_metrics = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "test_acc = test_metrics['acc']\n",
        "test_micro = test_metrics['f1_micro']\n",
        "test_macro = test_metrics['f1_macro']\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Test F1 Micro: {test_micro*100:.2f}% | Test F1 Macro: {test_macro*100:.2f}%')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.346 | Test Acc: 54.87% | Test F1 Micro: 66.52% | Test F1 Macro: 46.12%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:664: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaJzLdwb6exr",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ6W1Xpq6aO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emotion(model, tokenizer, tweet):\n",
        "  preds = []\n",
        "  model.eval()\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  predictions, attn_weights = model(tensor)\n",
        "  preds.append(torch.sigmoid(predictions).detach().cpu().numpy())\n",
        "  return preds, attn_weights, tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL9EtxTrG7My",
        "colab_type": "text"
      },
      "source": [
        "Lets test the model on our own input and save the attention weights and tokens for visualization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eecw3IVA6lVH",
        "colab_type": "code",
        "outputId": "af1868c7-d509-4a66-c908-6031097a3325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "preds, attn_weights, tokens = predict_emotion(model, tokenizer, \n",
        "                                              \"Good music, I love that shit.\")\n",
        "\n",
        "vals = []\n",
        "for p in preds[0]:\n",
        "  for val in p:\n",
        "    vals.append(val)\n",
        "\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "  print(f\"{label.upper()}: {vals[i]}\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANGER: 0.3511693477630615\n",
            "ANTICIPATION: 0.12626828253269196\n",
            "DISGUST: 0.2717917561531067\n",
            "FEAR: 0.1034935787320137\n",
            "JOY: 0.8128243088722229\n",
            "LOVE: 0.48896071314811707\n",
            "OPTIMISM: 0.559106171131134\n",
            "PESSIMISM: 0.06569769978523254\n",
            "SADNESS: 0.11318326741456985\n",
            "SURPRISE: 0.09305175393819809\n",
            "TRUST: 0.2579237222671509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jeDWh8KHbEG",
        "colab_type": "text"
      },
      "source": [
        "Here we format the attention weights and store the results in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q71LaaAn6oBA",
        "colab_type": "code",
        "outputId": "b6b8a1af-c62e-44e2-af91-265c8a0bf6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "aws = []\n",
        "for a in attn_weights[0]:\n",
        "  for v in a:\n",
        "    aws.append(v.detach().cpu().numpy())\n",
        "\n",
        "aws = aws[1:-1]\n",
        "aws = np.array(aws)\n",
        "aws"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06429043, 0.01932916, 0.02743982, 0.04082123, 0.67441005,\n",
              "       0.02125979, 0.13989049, 0.00411803], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnrDFOGy9DLx",
        "colab_type": "code",
        "outputId": "ca3568a3-efe7-4f3c-88cd-aa22acc78c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "attn_dict = {}\n",
        "for i in range(len(aws)):\n",
        "  attn_dict[tokens[i]] = aws[i]\n",
        "\n",
        "print(attn_dict)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'good': 0.064290434, 'music': 0.019329157, ',': 0.02743982, 'i': 0.04082123, 'love': 0.67441005, 'that': 0.021259792, 'shit': 0.13989049, '.': 0.004118027}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomju0TvHp8z",
        "colab_type": "text"
      },
      "source": [
        "Lets return the top 3 words that the model focused on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skdcPb0g-nQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LqBqNKVCqan",
        "colab_type": "code",
        "outputId": "2e7bb572-cc0d-4d94-9e0a-898db7eadd6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Counter(attn_dict).most_common(3)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('love', 0.67441005), ('shit', 0.13989049), ('good', 0.064290434)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    }
  ]
}