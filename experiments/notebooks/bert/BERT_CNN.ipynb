{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIg1HShVXH19bDXIOW663o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/BERT_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owG-IxGmt6PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjzXVnDxtlRF",
        "colab_type": "code",
        "outputId": "c8fa0a93-7ab8-4568-f86f-2d131eada6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N2UGYjytpuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "819dc07e-f5c0-4dcb-b870-f4af32cdf355"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from torchtext import data\n",
        "from transformers import BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3-e1jPqoJnc",
        "colab_type": "text"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0j_RCS0oLhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    \"bert_tokenizer\": \"bert-base-uncased\",\n",
        "    \"bert_pretrained_model\": \"bert-base-uncased\",\n",
        "    \"distilbert_tokenizer\": \"distilbert-base-uncased\",\n",
        "    \"distilbert_pretrained_model\": \"distilbert-base-uncased\",\n",
        "    \"seed\": 1234,\n",
        "    \"batch_size\": 64,\n",
        "    \"num_filters\": 100,\n",
        "    \"filter_sizes\": [3,4,5],\n",
        "    \"output_dim\": 11,\n",
        "    \"dropout\": 0.5,\n",
        "    \"epochs\": 10\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTPBR-lIovsI",
        "colab_type": "text"
      },
      "source": [
        "# Bert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR5RJneu1sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(args['bert_tokenizer'])\n",
        "#tokenizer = DistilBertTokenizer.from_pretrained(args['distilbert_tokenizer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro_0OHbpu8G9",
        "colab_type": "code",
        "outputId": "3e057f80-9d8d-4ae4-a6f8-3ad387d9f32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeyvkR7XAqH4",
        "colab_type": "code",
        "outputId": "c228aa27-4109-41b6-d3f7-d66e6f7b85f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEEo_OG7v4IN",
        "colab_type": "code",
        "outputId": "67cea376-d01b-478e-9394-5ffefedc7435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yUzVdafvcpC",
        "colab_type": "code",
        "outputId": "ee3df9ef-78cd-4655-f84c-8fd8a8e63c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes[args['bert_tokenizer']]\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jx93Gzou92U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(tweet):\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdXOgr6bo_si",
        "colab_type": "text"
      },
      "source": [
        "# Load & Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwTeWoXTukDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive'\n",
        "\n",
        "DATA_PATH = Path(file_path + '/datasets/SemEval')\n",
        "\n",
        "random.seed(args['seed'])\n",
        "np.random.seed(args['seed'])\n",
        "torch.manual_seed(args['seed'])\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential = False,\n",
        "                        use_vocab = False,\n",
        "                        pad_token= None,\n",
        "                        unk_token = None, \n",
        "                        dtype = torch.float)\n",
        "\n",
        "dataFields = {\"Tweet\": (\"Tweet\", TEXT),\n",
        "              'anger': (\"anger\", LABEL),\n",
        "              'anticipation': (\"anticipation\", LABEL),\n",
        "              'disgust': (\"disgust\", LABEL),\n",
        "              'fear': (\"fear\", LABEL),\n",
        "              'joy': (\"joy\", LABEL),\n",
        "              'love': (\"love\", LABEL),\n",
        "              'optimism': (\"optimism\", LABEL),\n",
        "              'pessimism': (\"pessimism\", LABEL),\n",
        "              'sadness': (\"sadness\", LABEL),\n",
        "              'surprise': (\"surprise\", LABEL),\n",
        "              'trust': (\"trust\", LABEL)}\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path = DATA_PATH,\n",
        "    train = 'train.csv',\n",
        "    validation = 'val.csv',\n",
        "    test = 'test.csv',\n",
        "    format = 'csv',\n",
        "    fields = dataFields\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0HXAS8EwK7z",
        "colab_type": "code",
        "outputId": "3af9c595-521b-4959-99b0-fb5c98823f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 6838\n",
            "Number of validation examples: 886\n",
            "Number of testing examples: 3259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz15EhbHwlTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = args['batch_size'],\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf0eDnfJxEmF",
        "colab_type": "text"
      },
      "source": [
        "# Build Vocab for Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J749TkJ-Hth_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "              'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
        "\n",
        "iaux = 0\n",
        "\n",
        "for batch in valid_iterator:\n",
        "  iaux += 1\n",
        "  aux = batch\n",
        "  aux2 = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "  if aux == 20: break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdVG6iO5xYSL",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Os2Jg6dxcHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained(args['bert_pretrained_model'])\n",
        "#bert = DistilBertModel.from_pretrained(args['distilbert_pretrained_model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ispEw6-G5PHN",
        "colab_type": "code",
        "outputId": "32acad23-1f79-4357-b1fa-c7b2fdc09c93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "bert.embeddings"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertEmbeddings(\n",
              "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "  (position_embeddings): Embedding(512, 768)\n",
              "  (token_type_embeddings): Embedding(2, 768)\n",
              "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe2-uC23xxhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertCNN(nn.Module):\n",
        "  def __init__(self, bert, n_filters, filter_sizes, output_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.bert = bert \n",
        "\n",
        "    embedding_dim = 768\n",
        "\n",
        "    self.convs = nn.ModuleList([\n",
        "                                nn.Conv2d(in_channels = 1,\n",
        "                                          out_channels = n_filters,\n",
        "                                          kernel_size = (fs, embedding_dim)) \n",
        "                                for fs in filter_sizes])\n",
        "    \n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, text):\n",
        "    with torch.no_grad():\n",
        "      embedded = self.bert(text)[0]\n",
        "    \n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    \n",
        "    conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    \n",
        "    cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    \n",
        "    return self.fc(cat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjxJgdC0umb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertCNN(bert=bert, \n",
        "                n_filters=args['num_filters'], \n",
        "                filter_sizes=args['filter_sizes'], \n",
        "                output_dim=args['output_dim'], \n",
        "                dropout=args['dropout'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKP5_3J30-qX",
        "colab_type": "code",
        "outputId": "06a9f732-d138-462d-f6d6-1502e3ded692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertCNN(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(3, 768), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(4, 768), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(5, 768), stride=(1, 1))\n",
              "  )\n",
              "  (fc): Linear(in_features=300, out_features=11, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXDfpNUN0_Ns",
        "colab_type": "code",
        "outputId": "75ea57a5-dedd-4926-b3ca-015f1b01c4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 110,407,451 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJmHEDeR1IUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMEjhKzy1MpT",
        "colab_type": "code",
        "outputId": "b634afcf-0ded-4ccc-9ab6-bfdf2e88f142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 925,211 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vMffCB91RYi",
        "colab_type": "code",
        "outputId": "e7eca760-357e-488a-a5f0-c7d5fa6c0b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convs.0.weight\n",
            "convs.0.bias\n",
            "convs.1.weight\n",
            "convs.1.bias\n",
            "convs.2.weight\n",
            "convs.2.bias\n",
            "fc.weight\n",
            "fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TVLrvFb1Zch",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46iK1uys1Wmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQWnKk5m1eUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "07941820-f904-450b-c9a3-cb89e4188fb7"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "label_weight = [1/11]*11\n",
        "\n",
        "label_weights = torch.tensor(label_weight).to(device)\n",
        "print(label_weights)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(weight=label_weights)\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,\n",
            "        0.0909, 0.0909], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63IKcBC31rMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBCzL0Z-1vN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metricize(preds, y):\n",
        "  global var_y\n",
        "  global var_preds\n",
        "  var_y = y\n",
        "  var_preds = preds\n",
        "  f1_macro = f1_score(y, preds.round(), average='macro')\n",
        "  f1_micro = f1_score(y, preds.round(), average='micro')\n",
        "  acc = roc_auc_score(y, preds)\n",
        "\n",
        "  return {\n",
        "      'f1_macro': f1_macro,\n",
        "      'f1_micro': f1_micro,\n",
        "      'acc': acc\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vtKoYNR1xbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    predictions = model(batch.Tweet).squeeze(1)\n",
        "\n",
        "    batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "    batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "    loss = criterion(predictions, batch_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "    labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nthY6Knv14uC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions = model(batch.Tweet).squeeze(1)\n",
        "\n",
        "      batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "      batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "      loss = criterion(predictions, batch_labels)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "      labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrqsUMQY17tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJRKTout1_BJ",
        "colab_type": "code",
        "outputId": "c0260168-32c5-4f40-acae-393260c04173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_metrics = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_metrics = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_history.append(train_loss)\n",
        "    valid_history.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'bert-cnn-model.pt')\n",
        "\n",
        "    train_acc = train_metrics['acc']\n",
        "    train_micro = train_metrics['f1_micro']\n",
        "    train_macro = train_metrics['f1_macro']\n",
        "\n",
        "    valid_acc = valid_metrics['acc']\n",
        "    valid_micro = valid_metrics['f1_micro']\n",
        "    valid_macro = valid_metrics['f1_macro']\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train F1 Micro: {train_micro*100:.2f}% | Train F1 Macro: {train_macro*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%  | Val. F1 Micro: {valid_micro*100:.2f}%  | Val. F1 Macro: {valid_macro*100:.2f}%')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.039 | Train Acc: 67.73% | Train F1 Micro: 42.16% | Train F1 Macro: 25.16%\n",
            "\t Val. Loss: 0.032 | Val. Acc: 82.13%  | Val. F1 Micro: 56.92%  | Val. F1 Macro: 34.69%\n",
            "Epoch: 02 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.033 | Train Acc: 79.50% | Train F1 Micro: 56.83% | Train F1 Macro: 37.17%\n",
            "\t Val. Loss: 0.031 | Val. Acc: 83.75%  | Val. F1 Micro: 59.98%  | Val. F1 Macro: 38.81%\n",
            "Epoch: 03 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.031 | Train Acc: 82.56% | Train F1 Micro: 60.64% | Train F1 Macro: 41.48%\n",
            "\t Val. Loss: 0.030 | Val. Acc: 84.40%  | Val. F1 Micro: 62.06%  | Val. F1 Macro: 42.22%\n",
            "Epoch: 04 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.029 | Train Acc: 84.20% | Train F1 Micro: 62.94% | Train F1 Macro: 45.07%\n",
            "\t Val. Loss: 0.029 | Val. Acc: 84.63%  | Val. F1 Micro: 64.22%  | Val. F1 Macro: 45.44%\n",
            "Epoch: 05 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.028 | Train Acc: 85.71% | Train F1 Micro: 64.51% | Train F1 Macro: 47.47%\n",
            "\t Val. Loss: 0.029 | Val. Acc: 84.61%  | Val. F1 Micro: 65.32%  | Val. F1 Macro: 46.70%\n",
            "Epoch: 06 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.027 | Train Acc: 86.54% | Train F1 Micro: 66.06% | Train F1 Macro: 49.13%\n",
            "\t Val. Loss: 0.030 | Val. Acc: 84.40%  | Val. F1 Micro: 64.95%  | Val. F1 Macro: 46.87%\n",
            "Epoch: 07 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.027 | Train Acc: 87.33% | Train F1 Micro: 66.90% | Train F1 Macro: 50.92%\n",
            "\t Val. Loss: 0.029 | Val. Acc: 84.47%  | Val. F1 Micro: 67.00%  | Val. F1 Macro: 46.47%\n",
            "Epoch: 08 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.026 | Train Acc: 88.29% | Train F1 Micro: 68.32% | Train F1 Macro: 52.61%\n",
            "\t Val. Loss: 0.029 | Val. Acc: 84.48%  | Val. F1 Micro: 65.09%  | Val. F1 Macro: 45.63%\n",
            "Epoch: 09 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.025 | Train Acc: 89.06% | Train F1 Micro: 69.44% | Train F1 Macro: 53.98%\n",
            "\t Val. Loss: 0.029 | Val. Acc: 84.43%  | Val. F1 Micro: 65.98%  | Val. F1 Macro: 46.47%\n",
            "Epoch: 10 | Epoch Time: 0m 15s\n",
            "\tTrain Loss: 0.024 | Train Acc: 89.78% | Train F1 Micro: 70.28% | Train F1 Macro: 55.65%\n",
            "\t Val. Loss: 0.029 | Val. Acc: 84.35%  | Val. F1 Micro: 65.95%  | Val. F1 Macro: 47.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Y6H4Bg_H02",
        "colab_type": "text"
      },
      "source": [
        "# Plot Training & Validation History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApekxv62E2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYvkc4ylPU2H",
        "colab_type": "code",
        "outputId": "49824551-c115-414a-fd8b-51490b0e86ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(valid_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training', 'Validation'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5d806b2080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR5X3c8r_W1m",
        "colab_type": "text"
      },
      "source": [
        "# Model results on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI8O3g5_PY1l",
        "colab_type": "code",
        "outputId": "a88927fe-8dd7-4c1a-dcd2-0c27869f2d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('bert-cnn-model.pt'))\n",
        "\n",
        "test_loss, test_metrics = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "test_acc = test_metrics['acc']\n",
        "test_micro = test_metrics['f1_micro']\n",
        "test_macro = test_metrics['f1_macro']\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Test F1 Micro: {test_micro*100:.2f}% | Test F1 Macro: {test_macro*100:.2f}%')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.029 | Test Acc: 84.08% | Test F1 Micro: 66.85% | Test F1 Macro: 46.30%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvYzUzo73i4",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldd6LfSYRJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emotion(model, tokenizer, tweet):\n",
        "  preds = []\n",
        "  model.eval()\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  predictions = model(tensor)\n",
        "  preds.append(torch.sigmoid(predictions).detach().cpu().numpy())\n",
        "  #preds += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0htQkUGZKmS",
        "colab_type": "code",
        "outputId": "a87c26bf-76d0-4a89-98db-fe3ba27f5cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "LABEL_COLS"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger',\n",
              " 'anticipation',\n",
              " 'disgust',\n",
              " 'fear',\n",
              " 'joy',\n",
              " 'love',\n",
              " 'optimism',\n",
              " 'pessimism',\n",
              " 'sadness',\n",
              " 'surprise',\n",
              " 'trust']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRlr-s2DY8Df",
        "colab_type": "code",
        "outputId": "1eabd0ae-3d50-4be0-a704-623be1cb0e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "preds = predict_emotion(model, tokenizer, \"I am looking forward to tomorrow, cant wait!\")\n",
        "\n",
        "vals = []\n",
        "for p in preds[0]:\n",
        "  for val in p:\n",
        "    vals.append(val)\n",
        "\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "  print(f\"{label.upper()}: {vals[i]}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANGER: 0.1052500456571579\n",
            "ANTICIPATION: 0.5501772165298462\n",
            "DISGUST: 0.07675481587648392\n",
            "FEAR: 0.20253396034240723\n",
            "JOY: 0.7021611332893372\n",
            "LOVE: 0.04384106025099754\n",
            "OPTIMISM: 0.5876724720001221\n",
            "PESSIMISM: 0.05829329788684845\n",
            "SADNESS: 0.11524981260299683\n",
            "SURPRISE: 0.04783833026885986\n",
            "TRUST: 0.05171003192663193\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}