{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNczn/0i4CjD2/0K37IujEu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaarnikoivu/dissertation/blob/master/Attention_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaTkjkKu9TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy40NOL-vFij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6a32a5e6-5b4a-4f36-fa0e-484cf8bbd1e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xU5Pc6mvbY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "24f68d46-515a-4fb7-d7fe-25727f4613e8"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from torchtext import data\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FczMGWp1vM_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    \"bert_tokenizer\": \"bert-base-uncased\",\n",
        "    \"bert_pretrained_model\": \"bert-base-uncased\",\n",
        "    \"seed\": 1234,\n",
        "    \"batch_size\": 64,\n",
        "    \"num_filters\": 100,\n",
        "    \"filter_sizes\": [3,4,5],\n",
        "    \"output_dim\": 11,\n",
        "    \"dropout\": 0.5,\n",
        "    \"epochs\": 2\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4nYNxu4vV9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(args['bert_tokenizer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqDxLYBQvYc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c141e73-e4ff-43d8-fde5-181d29578227"
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDPOUfFuvjvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8142496a-f60e-48a0-d92a-3bdd523967d1"
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBv0eC2xvss-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88474092-a99b-4463-ed1b-30bde9d81b2f"
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes[args['bert_tokenizer']]\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMBjmcymvlY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_cut(tweet):\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYj3qsDpvmwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive'\n",
        "\n",
        "DATA_PATH = Path(file_path + '/datasets/SemEval')\n",
        "\n",
        "random.seed(args['seed'])\n",
        "np.random.seed(args['seed'])\n",
        "torch.manual_seed(args['seed'])\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(sequential = False,\n",
        "                        use_vocab = False,\n",
        "                        pad_token= None,\n",
        "                        unk_token = None, \n",
        "                        dtype = torch.float)\n",
        "\n",
        "dataFields = {\"Tweet\": (\"Tweet\", TEXT),\n",
        "              'anger': (\"anger\", LABEL),\n",
        "              'anticipation': (\"anticipation\", LABEL),\n",
        "              'disgust': (\"disgust\", LABEL),\n",
        "              'fear': (\"fear\", LABEL),\n",
        "              'joy': (\"joy\", LABEL),\n",
        "              'love': (\"love\", LABEL),\n",
        "              'optimism': (\"optimism\", LABEL),\n",
        "              'pessimism': (\"pessimism\", LABEL),\n",
        "              'sadness': (\"sadness\", LABEL),\n",
        "              'surprise': (\"surprise\", LABEL),\n",
        "              'trust': (\"trust\", LABEL)}\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path = DATA_PATH,\n",
        "    train = 'train.csv',\n",
        "    validation = 'val.csv',\n",
        "    test = 'test.csv',\n",
        "    format = 'csv',\n",
        "    fields = dataFields\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYMezWxUvyxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    sort_key = lambda x: len(x.Tweet),\n",
        "    sort_within_batch = True,\n",
        "    batch_size = args['batch_size'],\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZCExSovoy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLS = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
        "              'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
        "\n",
        "iaux = 0\n",
        "\n",
        "for batch in valid_iterator:\n",
        "  iaux += 1\n",
        "  aux = batch\n",
        "  aux2 = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "  if aux == 20: break;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRiJ1llOvvxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert = BertModel.from_pretrained(args['bert_pretrained_model'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD1MiUiwv29n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.tanh(x)\n",
        "    x = self.attention(x).squeeze(2)\n",
        "    alpha = F.softmax(x, dim=1).unsqueeze(1)\n",
        "    return alpha\n",
        "\n",
        "class AttentionBiLSTM(nn.Module):\n",
        "  def __init__(self, bert, hidden_size, num_layers, dropout, fc_dropout, \n",
        "               embed_dropout, num_classes):\n",
        "    super(AttentionBiLSTM, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size \n",
        "    self.bert = bert \n",
        "    embedding_dim = 768 \n",
        "\n",
        "    self.embed_dropout = nn.Dropout(embed_dropout)\n",
        "\n",
        "    self.bilstm = nn.LSTM(embedding_dim, \n",
        "                          hidden_size, \n",
        "                          num_layers, \n",
        "                          dropout=(0 if num_layers==1 else dropout),\n",
        "                          bidirectional=True,\n",
        "                          batch_first=True)\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    self.fc_dropout = nn.Dropout(fc_dropout)\n",
        "    \n",
        "    self.attention = Attention(hidden_size)\n",
        "  \n",
        "  def forward(self, text):\n",
        "    with torch.no_grad():\n",
        "      x = self.bert(text)[0]\n",
        "    \n",
        "    x = self.embed_dropout(x)\n",
        "    y, _ = self.bilstm(x)\n",
        "    y = y[:,:,:self.hidden_size] + y[:,:,self.hidden_size:]\n",
        "    alpha = self.attention(y)\n",
        "    r = alpha.bmm(y).squeeze(1)\n",
        "    h = torch.tanh(r)\n",
        "    logits = self.fc(h)\n",
        "    logits = self.fc_dropout(logits)\n",
        "    return logits, alpha "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukvWtx9gxE1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83b18ab0-d9b5-4c9d-e027-00d23ed054a3"
      },
      "source": [
        "model = AttentionBiLSTM(\n",
        "    bert=bert,\n",
        "    hidden_size=768,\n",
        "    num_layers=2,\n",
        "    dropout=0.5,\n",
        "    fc_dropout=0.5,\n",
        "    embed_dropout=0.2,\n",
        "    num_classes=11,\n",
        ")\n",
        "\n",
        "model"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttentionBiLSTM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (embed_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (bilstm): LSTM(768, 768, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=768, out_features=11, bias=True)\n",
              "  (fc_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (attention): Attention(\n",
              "    (attention): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4i0FWYtxQ0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1af7wW5xYk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "15d5a2d1-29b9-46be-9e15-aec74f31af5b"
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bilstm.weight_ih_l0\n",
            "bilstm.weight_hh_l0\n",
            "bilstm.bias_ih_l0\n",
            "bilstm.bias_hh_l0\n",
            "bilstm.weight_ih_l0_reverse\n",
            "bilstm.weight_hh_l0_reverse\n",
            "bilstm.bias_ih_l0_reverse\n",
            "bilstm.bias_hh_l0_reverse\n",
            "bilstm.weight_ih_l1\n",
            "bilstm.weight_hh_l1\n",
            "bilstm.bias_ih_l1\n",
            "bilstm.bias_hh_l1\n",
            "bilstm.weight_ih_l1_reverse\n",
            "bilstm.weight_hh_l1_reverse\n",
            "bilstm.bias_ih_l1_reverse\n",
            "bilstm.bias_hh_l1_reverse\n",
            "fc.weight\n",
            "fc.bias\n",
            "attention.attention.weight\n",
            "attention.attention.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1CpxbIDxcb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwZIAAQmxfxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxF1sRahxmwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YydgwAqkxolz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metricize(preds, y):\n",
        "  f1_macro = f1_score(y, preds.round(), average='macro')\n",
        "  f1_micro = f1_score(y, preds.round(), average='micro')\n",
        "  acc = roc_auc_score(y, preds)\n",
        "\n",
        "  return {\n",
        "      'f1_macro': f1_macro,\n",
        "      'f1_micro': f1_micro,\n",
        "      'acc': acc\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLL3mSPZxpkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions, _ = model(batch.Tweet)\n",
        "\n",
        "    batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "    batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "    loss = criterion(predictions, batch_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "    labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoLaQBVvxrla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  preds_list = []\n",
        "  labels_list = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "      predictions, _ = model(batch.Tweet)\n",
        "\n",
        "      batch_labels = torch.stack([getattr(batch, label) for label in LABEL_COLS])\n",
        "      batch_labels = torch.transpose(batch_labels, 0, 1)\n",
        "\n",
        "      loss = criterion(predictions, batch_labels)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      preds_list += [torch.sigmoid(predictions).detach().cpu().numpy()]\n",
        "      labels_list += [batch_labels.cpu().numpy()]\n",
        "\n",
        "  return epoch_loss / len(iterator), metricize(np.vstack(preds_list),\n",
        "                                             np.vstack(labels_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO9vUx_1xtXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSBSFoBOxwJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7ac5c88e-f909-454c-efa4-daabd262c095"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_history = []\n",
        "valid_history = []\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_metrics = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_metrics = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    train_history.append(train_loss)\n",
        "    valid_history.append(valid_loss)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'bert-lstm-model.pt')\n",
        "\n",
        "    train_acc = train_metrics['acc']\n",
        "    train_micro = train_metrics['f1_micro']\n",
        "    train_macro = train_metrics['f1_macro']\n",
        "\n",
        "    valid_acc = valid_metrics['acc']\n",
        "    valid_micro = valid_metrics['f1_micro']\n",
        "    valid_macro = valid_metrics['f1_macro']\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train F1 Micro: {train_micro*100:.2f}% | Train F1 Macro: {train_macro*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%  | Val. F1 Micro: {valid_micro*100:.2f}%  | Val. F1 Macro: {valid_macro*100:.2f}%')"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 22s\n",
            "\tTrain Loss: 0.552 | Train Acc: 59.61% | Train F1 Micro: 28.68% | Train F1 Macro: 17.84%\n",
            "\t Val. Loss: 0.427 | Val. Acc: 79.59%  | Val. F1 Micro: 58.15%  | Val. F1 Macro: 36.82%\n",
            "Epoch: 02 | Epoch Time: 0m 22s\n",
            "\tTrain Loss: 0.524 | Train Acc: 64.32% | Train F1 Micro: 35.29% | Train F1 Macro: 23.54%\n",
            "\t Val. Loss: 0.393 | Val. Acc: 83.27%  | Val. F1 Micro: 61.21%  | Val. F1 Macro: 39.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CePfv7_0An7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgUKd4HR6Tg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "1039c5c2-e2ac-4528-ac0b-d279c84cdcc8"
      },
      "source": [
        "plt.plot(train_history)\n",
        "plt.plot(valid_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training', 'Validation'])"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3109295978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nECIiEksBQkREYilAiIhILAUIERGJ9f8B1KRQ3M/+YXwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TxQGreq6Vgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "119ea5b7-ec68-48eb-ca15-ffcafd70d8f8"
      },
      "source": [
        "model.load_state_dict(torch.load('bert-lstm-model.pt'))\n",
        "\n",
        "test_loss, test_metrics = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "test_acc = test_metrics['acc']\n",
        "test_micro = test_metrics['f1_micro']\n",
        "test_macro = test_metrics['f1_macro']\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% | Test F1 Micro: {test_micro*100:.2f}% | Test F1 Macro: {test_macro*100:.2f}%')"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.390 | Test Acc: 82.74% | Test F1 Micro: 61.77% | Test F1 Macro: 40.60%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaJzLdwb6exr",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ6W1Xpq6aO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_emotion(model, tokenizer, tweet):\n",
        "  preds = []\n",
        "  model.eval()\n",
        "  tokens = tokenizer.tokenize(tweet)\n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(0)\n",
        "  predictions, attn_weights = model(tensor)\n",
        "  preds.append(torch.sigmoid(predictions).detach().cpu().numpy())\n",
        "  return preds, attn_weights, tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eecw3IVA6lVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d9e97b69-b378-4965-fef9-07cdeda44eaa"
      },
      "source": [
        "preds, attn_weights, tokens = predict_emotion(model, tokenizer, \"Absolutely loving life right now!\")\n",
        "\n",
        "vals = []\n",
        "for p in preds[0]:\n",
        "  for val in p:\n",
        "    vals.append(val)\n",
        "\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "  print(f\"{label.upper()}: {vals[i]}\")"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANGER: 0.27142301201820374\n",
            "ANTICIPATION: 0.2915802001953125\n",
            "DISGUST: 0.25697723031044006\n",
            "FEAR: 0.11242149770259857\n",
            "JOY: 0.8250339031219482\n",
            "LOVE: 0.42577651143074036\n",
            "OPTIMISM: 0.5451579689979553\n",
            "PESSIMISM: 0.10848168283700943\n",
            "SADNESS: 0.1736668199300766\n",
            "SURPRISE: 0.42238131165504456\n",
            "TRUST: 0.367700457572937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q71LaaAn6oBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3546db2c-abef-4ccd-8b7b-c78284d3284c"
      },
      "source": [
        "aws = []\n",
        "for a in attn_weights[0]:\n",
        "  for v in a:\n",
        "    aws.append(v.detach().cpu().numpy())\n",
        "\n",
        "aws = aws[1:-1]\n",
        "aws = np.array(aws)\n",
        "aws"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1616258 , 0.3832476 , 0.13853529, 0.10852786, 0.06967954,\n",
              "       0.08636736], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnrDFOGy9DLx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75e3ee72-61d0-4d69-eefc-b3cd99aa55ca"
      },
      "source": [
        "my_dict = {}\n",
        "for i in range(len(aws)):\n",
        "  my_dict[tokens[i]] = aws[i]\n",
        "\n",
        "print(my_dict)"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'absolutely': 0.1616258, 'loving': 0.3832476, 'life': 0.13853529, 'right': 0.10852786, 'now': 0.06967954, '!': 0.08636736}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skdcPb0g-nQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LqBqNKVCqan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35baf300-2c31-4edb-8bde-7cc09b2fc2a3"
      },
      "source": [
        "Counter(my_dict).most_common(3)"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('loving', 0.3832476), ('absolutely', 0.1616258), ('life', 0.13853529)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNRWD70OCv9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}