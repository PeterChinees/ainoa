\chapter{Experiment Design}\label{ch:Requirements Analysis}

This chapter details the experimental setup for carrying out the task of classifying emotion from text. In particular, the chapter outlines the text pre-processing steps, the model architecture, evaluation metrics and technologies that are made use of.

\section{Data}

This study makes use of the ISEAR dataset (International Survey on Emotion Antecedents And Reactions) which consists of 3000 responses from people around the world who report situations where they experienced each of the emotions from (joy, fear, anger, sadness, disgust, shame, and guilt). The dataset is split such that 80\% is assigned to the training set, 10\% to the validation set and the last 10\% to the testing set. 

\subsection{Pre-trained Word Embeddings \& Language Models}

Due to the minimal amount of training data, we make use of the pre-trained GloVe embeddings trained on the Wikipedia and Gigaword corpus \textit{(6B tokens, 400k vocab, uncased, 300d)}, the pre-trained Word2Vec embeddings trained on the Google News corpus \textit{(100B words, 3M vocab, 300d)} as well as the ELMo embeddings trained on the 1 Billion word benchmark available from \href{https://tfhub.dev/}{Tensorflow-Hub}. Furthermore, the \href{https://github.com/huggingface/transformers}{Huggingface Transformers} library for Tensorflow is utilized in order to retrieve the pre-trained BERT model and to compute BERT embeddings. 

\subsection{Pre-processing}

Text in which people recall experiences likely consists of various informal words and words which don't have any significant correlation towards the emotions being conveyed. Thus, keeping these words in the text increases the overall feature dimensionality. As a means to counter this issue, this study makes use of text pre-processing techniques such as stop-word removal, removing punctuations and accent marks, removing white spaces, and likewise to \citep{DBLP:journals/corr/abs-1801-06146}, adding special tokens for upper-case words and repetition as this may allow the model to capture characteristics which are important to classification.

\section{Models}

As shown by the enumerated list below, the overall model architecture is designed such that it contains the following layers: 

\begin{enumerate}
  \item Pre-trained embeddings layer 
  \item Model layer 
  \item Dropout layer 
  \item Classification layer 
\end{enumerate}

\noindent
The embedding layer is initialized using pre-trained Word2Vec, ELMo and BERT embeddings such that the pre-processed input text from the dataset is converted into an embedding representation. Furthermore, the model layer makes use of an RNN, CNN and RCNN. With regards to the RNN, the experiment is carried out using both a unidirectional LSTM as well as a bidirectional LSTM. The RCNN architecture proposed by \citep{AAAI159745} is used in order to take advantage of both the RNN and CNN as a means to further improve the emotion classification. Next, the dropout layer is implemented in order to ensure that the model does not over-fit the data. Lastly, the classification layer makes use of Cross-Entropy loss in order to compute the probabilities of the input text belonging to each of the class labels. 

\begin{figure}[H]
  \centering
  \includegraphics*[ width=1\linewidth]{methodology/images/design.png}
  \caption{Proposed architecture.}
  \label{fig:bert-model}
\end{figure}

\section{Performance Evaluation}

This study evaluates the performance of all models using accuracy and the micro and macro F1 scores. We specifically optimize each model with the micro and macro F1 scores in mind. This is because the dataset consists of labels with more instances than others. Furthermore, the micro and macro F1 scores have been used in various other papers that tackle the task of emotion classification from text \citep{paperswithcode}. Therefore, these metrics provide a good comparison for the overall performance of the models implemented in this study. 

\begin{equation}
  PREmicro = \frac{TP1 + TP2}{TP1 + TP2 + FP1 + FP2}
\end{equation}

\begin{equation}
  RECmicro = \frac{TP1 + TP2}{TP1 + TP2 + FN1 + FN2}
\end{equation}

\noindent
\\ The micro F1 score is equal to the harmonic mean of these two figures \citep{shams_1970}.

\begin{equation}
  PREmacro = \frac{PRE1 + PRE2}{2}
\end{equation}

\begin{equation}
  RECmacro = \frac{REC1 + REC2}{2}
\end{equation}

\noindent
\\ The macro F1 score is equal to the harmonic mean of these two figures \citep{shams_1970}. 

\section{Technologies}

The experiment makes use of Python along with Tensorflow and Keras for constructing the architecture of the models. Both the Natural Language Toolkit (NLTK), and Keras Tokenizer libraries are used for text pre-processing. The Scikit learn and Pandas libraries are used for data analysis whilst Matplotlib is used for data visualization. 





  